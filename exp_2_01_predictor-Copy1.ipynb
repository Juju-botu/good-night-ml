{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from datetime import date\n",
    "import datetime\n",
    "from datetime import timedelta  \n",
    "import csv\n",
    "import holidays # for importing the public holidays\n",
    "import re\n",
    "import torch\n",
    "from src.utils import *\n",
    "from src.data_miner import DataMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 5\n",
    "min_hour = 21 # Minimum hour for sleep detection\n",
    "max_hour = 5\n",
    "train_window = 3 # Sequence length\n",
    "local_holidays = holidays.Italy(prov='BO') # Get the holidays in Bologna, Italy :)\n",
    "train_episodes = 500\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "data_dir = \"data\"\n",
    "dataset = \"data/LastSeenDataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature extraction: we first extract the features given the time series data of Telegram accesses.\n",
    "- Supposition: last Telegram access in very similar to the time the person goes to sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Possible features to extract: \n",
    "1. Last seen time (arguably the most important)\n",
    "2. Wake up time\n",
    "3. Number of Telegram accesses during the previous day\n",
    "4. Day of the week\n",
    "5. Public holiday presence in the following day (using the holidays library)\n",
    "6. (time spent on Telegram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First day is holiday:  False\n"
     ]
    }
   ],
   "source": [
    "with open(dataset, newline='') as csvfile:\n",
    "    date_list = list(csv.reader(csvfile))\n",
    "\n",
    "date_list = convert_to_dates(date_list)\n",
    "\n",
    "'''Test data: search calendar for local holidays'''\n",
    "print(\"First day is holiday: \", date_list[0][0] in local_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6002, 0.5434, 0.4465, 0.5033, 0.4888, 0.5380, 0.5200, 0.6680, 0.1981,\n",
      "         0.5418, 0.5891, 0.5230, 0.5878, 0.2870, 0.1545, 0.3483, 0.1007, 0.6694,\n",
      "         0.5091, 0.4906, 0.6093, 0.6412, 0.8530, 0.3883, 0.5664, 0.7656],\n",
      "        [0.6667, 0.5991, 0.6653, 0.6445, 0.7801, 0.6894, 0.6742, 0.6647, 1.0048,\n",
      "         0.6278, 0.7105, 0.6988, 0.6384, 0.8407, 0.9146, 0.7862, 1.1783, 0.4033,\n",
      "         0.6723, 0.6988, 0.6034, 0.5998, 0.5354, 0.7193, 0.7006, 0.5691],\n",
      "        [1.0000, 0.0000, 0.1667, 0.3333, 0.5000, 0.6667, 0.8333, 1.0000, 0.0000,\n",
      "         0.1667, 0.3333, 0.5000, 0.6667, 0.8333, 1.0000, 0.0000, 0.1667, 0.3333,\n",
      "         0.5000, 0.6667, 0.8333, 1.0000, 0.0000, 0.1667, 0.3333, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1400, 0.1200, 0.0600, 0.0500, 0.0700, 0.1400, 0.0967, 0.1400, 0.0300,\n",
      "         0.2100, 0.3400, 0.1400, 0.2600, 0.2200, 0.0500, 0.1700, 0.0300, 0.3900,\n",
      "         0.3600, 0.2500, 0.2600, 0.3300, 0.1400, 0.0900, 0.1300, 0.2000]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "data_tensor =  DataMiner(date_list).to_tensor(verbose=False)\n",
    "print(data_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the training data is not much, we can insert some noise to augment it; this will also make the model less prone to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "\n",
    "\n",
    "# We use the \"last 3\" trend\n",
    "# Credits: https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/\n",
    "'''The sequence on which we have a prediction is the last train_window days'''\n",
    "X, y = create_inout_sequences(data_tensor, train_window)\n",
    "X = X.transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- Time series data, so possible idea(s):\n",
    "    - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44649306 0.66525465 0.16666667 0.         0.06      ]\n",
      " [0.50329864 0.64453703 0.33333334 0.         0.05      ]\n",
      " [0.48875    0.7801157  0.5        0.         0.07      ]]\n",
      "[0.48875]\n"
     ]
    }
   ],
   "source": [
    "n_features = num_features # this is number of parallel inputs\n",
    "n_timesteps = train_window # this is number of timesteps\n",
    "\n",
    "# convert dataset into input/output\n",
    "\n",
    "# create NN\n",
    "mv_net = LSTM(n_features,n_timesteps)\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-1)\n",
    "\n",
    "print(X[2])\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    1   |   Loss: 0.141328 \n",
      "Step:    2   |   Loss: 0.020167 \n",
      "Step:    3   |   Loss: 0.043407 \n",
      "Step:    4   |   Loss: 0.041257 \n",
      "Step:    5   |   Loss: 0.022228 \n",
      "Step:    6   |   Loss: 0.034475 \n",
      "Step:    7   |   Loss: 0.025034 \n",
      "Step:    8   |   Loss: 0.025532 \n",
      "Step:    9   |   Loss: 0.029246 \n",
      "Step:   11   |   Loss: 0.030363 \n",
      "Step:   12   |   Loss: 0.030453 \n",
      "Step:   13   |   Loss: 0.030525 \n",
      "Step:   14   |   Loss: 0.024818 \n",
      "Step:   15   |   Loss: 0.029959 \n",
      "Step:   16   |   Loss: 0.028490 \n",
      "Step:   17   |   Loss: 0.022812 \n",
      "Step:   18   |   Loss: 0.030162 \n",
      "Step:   19   |   Loss: 0.028585 \n",
      "Step:   21   |   Loss: 0.028870 \n",
      "Step:   22   |   Loss: 0.027301 \n",
      "Step:   23   |   Loss: 0.027759 \n",
      "Step:   24   |   Loss: 0.027770 \n",
      "Step:   25   |   Loss: 0.029010 \n",
      "Step:   26   |   Loss: 0.026502 \n",
      "Step:   27   |   Loss: 0.028639 \n",
      "Step:   28   |   Loss: 0.026463 \n",
      "Step:   29   |   Loss: 0.025582 \n",
      "Step:   31   |   Loss: 0.026720 \n",
      "Step:   32   |   Loss: 0.026150 \n",
      "Step:   33   |   Loss: 0.025897 \n",
      "Step:   34   |   Loss: 0.024652 \n",
      "Step:   35   |   Loss: 0.027328 \n",
      "Step:   36   |   Loss: 0.026773 \n",
      "Step:   37   |   Loss: 0.029704 \n",
      "Step:   38   |   Loss: 0.024699 \n",
      "Step:   39   |   Loss: 0.028345 \n",
      "Step:   41   |   Loss: 0.026994 \n",
      "Step:   42   |   Loss: 0.024588 \n",
      "Step:   43   |   Loss: 0.026285 \n",
      "Step:   44   |   Loss: 0.024994 \n",
      "Step:   45   |   Loss: 0.028403 \n",
      "Step:   46   |   Loss: 0.024002 \n",
      "Step:   47   |   Loss: 0.025782 \n",
      "Step:   48   |   Loss: 0.025461 \n",
      "Step:   49   |   Loss: 0.026693 \n",
      "Step:   51   |   Loss: 0.028364 \n",
      "Step:   52   |   Loss: 0.023175 \n",
      "Step:   53   |   Loss: 0.026548 \n",
      "Step:   54   |   Loss: 0.024678 \n",
      "Step:   55   |   Loss: 0.027112 \n",
      "Step:   56   |   Loss: 0.025500 \n",
      "Step:   57   |   Loss: 0.026438 \n",
      "Step:   58   |   Loss: 0.023796 \n",
      "Step:   59   |   Loss: 0.025194 \n",
      "Step:   61   |   Loss: 0.027658 \n",
      "Step:   62   |   Loss: 0.024078 \n",
      "Step:   63   |   Loss: 0.026524 \n",
      "Step:   64   |   Loss: 0.020531 \n",
      "Step:   65   |   Loss: 0.030631 \n",
      "Step:   66   |   Loss: 0.024243 \n",
      "Step:   67   |   Loss: 0.028011 \n",
      "Step:   68   |   Loss: 0.020313 \n",
      "Step:   69   |   Loss: 0.029340 \n",
      "Step:   71   |   Loss: 0.028042 \n",
      "Step:   72   |   Loss: 0.020717 \n",
      "Step:   73   |   Loss: 0.027452 \n",
      "Step:   74   |   Loss: 0.020992 \n",
      "Step:   75   |   Loss: 0.023320 \n",
      "Step:   76   |   Loss: 0.022423 \n",
      "Step:   77   |   Loss: 0.025461 \n",
      "Step:   78   |   Loss: 0.018565 \n",
      "Step:   79   |   Loss: 0.027860 \n",
      "Step:   81   |   Loss: 0.027757 \n",
      "Step:   82   |   Loss: 0.021145 \n",
      "Step:   83   |   Loss: 0.026720 \n",
      "Step:   84   |   Loss: 0.026959 \n",
      "Step:   85   |   Loss: 0.023319 \n",
      "Step:   86   |   Loss: 0.023028 \n",
      "Step:   87   |   Loss: 0.028095 \n",
      "Step:   88   |   Loss: 0.019016 \n",
      "Step:   89   |   Loss: 0.022930 \n",
      "Step:   91   |   Loss: 0.023248 \n",
      "Step:   92   |   Loss: 0.020517 \n",
      "Step:   93   |   Loss: 0.023719 \n",
      "Step:   94   |   Loss: 0.019286 \n",
      "Step:   95   |   Loss: 0.024209 \n",
      "Step:   96   |   Loss: 0.022590 \n",
      "Step:   97   |   Loss: 0.024213 \n",
      "Step:   98   |   Loss: 0.025028 \n",
      "Step:   99   |   Loss: 0.019675 \n",
      "Step:  101   |   Loss: 0.015453 \n",
      "Step:  102   |   Loss: 0.024870 \n",
      "Step:  103   |   Loss: 0.015828 \n",
      "Step:  104   |   Loss: 0.031981 \n",
      "Step:  105   |   Loss: 0.017470 \n",
      "Step:  106   |   Loss: 0.017570 \n",
      "Step:  107   |   Loss: 0.032982 \n",
      "Step:  108   |   Loss: 0.014304 \n",
      "Step:  109   |   Loss: 0.018257 \n",
      "Step:  111   |   Loss: 0.019323 \n",
      "Step:  112   |   Loss: 0.025459 \n",
      "Step:  113   |   Loss: 0.026948 \n",
      "Step:  114   |   Loss: 0.016635 \n",
      "Step:  115   |   Loss: 0.030322 \n",
      "Step:  116   |   Loss: 0.019049 \n",
      "Step:  117   |   Loss: 0.025727 \n",
      "Step:  118   |   Loss: 0.016783 \n",
      "Step:  119   |   Loss: 0.018539 \n",
      "Step:  121   |   Loss: 0.017438 \n",
      "Step:  122   |   Loss: 0.026178 \n",
      "Step:  123   |   Loss: 0.015625 \n",
      "Step:  124   |   Loss: 0.019704 \n",
      "Step:  125   |   Loss: 0.023173 \n",
      "Step:  126   |   Loss: 0.013882 \n",
      "Step:  127   |   Loss: 0.018999 \n",
      "Step:  128   |   Loss: 0.026188 \n",
      "Step:  129   |   Loss: 0.012963 \n",
      "Step:  131   |   Loss: 0.023295 \n",
      "Step:  132   |   Loss: 0.012865 \n",
      "Step:  133   |   Loss: 0.021875 \n",
      "Step:  134   |   Loss: 0.022059 \n",
      "Step:  135   |   Loss: 0.013896 \n",
      "Step:  136   |   Loss: 0.014761 \n",
      "Step:  137   |   Loss: 0.012056 \n",
      "Step:  138   |   Loss: 0.010938 \n",
      "Step:  139   |   Loss: 0.015947 \n",
      "Step:  141   |   Loss: 0.027033 \n",
      "Step:  142   |   Loss: 0.023588 \n",
      "Step:  143   |   Loss: 0.013177 \n",
      "Step:  144   |   Loss: 0.014380 \n",
      "Step:  145   |   Loss: 0.020611 \n",
      "Step:  146   |   Loss: 0.021470 \n",
      "Step:  147   |   Loss: 0.012834 \n",
      "Step:  148   |   Loss: 0.010824 \n",
      "Step:  149   |   Loss: 0.018657 \n",
      "Step:  151   |   Loss: 0.010627 \n",
      "Step:  152   |   Loss: 0.011615 \n",
      "Step:  153   |   Loss: 0.014405 \n",
      "Step:  154   |   Loss: 0.017341 \n",
      "Step:  155   |   Loss: 0.016305 \n",
      "Step:  156   |   Loss: 0.012542 \n",
      "Step:  157   |   Loss: 0.015490 \n",
      "Step:  158   |   Loss: 0.011019 \n",
      "Step:  159   |   Loss: 0.016790 \n",
      "Step:  161   |   Loss: 0.013838 \n",
      "Step:  162   |   Loss: 0.010202 \n",
      "Step:  163   |   Loss: 0.005975 \n",
      "Step:  164   |   Loss: 0.012924 \n",
      "Step:  165   |   Loss: 0.010580 \n",
      "Step:  166   |   Loss: 0.014367 \n",
      "Step:  167   |   Loss: 0.014001 \n",
      "Step:  168   |   Loss: 0.008465 \n",
      "Step:  169   |   Loss: 0.011860 \n",
      "Step:  171   |   Loss: 0.010051 \n",
      "Step:  172   |   Loss: 0.006913 \n",
      "Step:  173   |   Loss: 0.011107 \n",
      "Step:  174   |   Loss: 0.007344 \n",
      "Step:  175   |   Loss: 0.004828 \n",
      "Step:  176   |   Loss: 0.015107 \n",
      "Step:  177   |   Loss: 0.011199 \n",
      "Step:  178   |   Loss: 0.005261 \n",
      "Step:  179   |   Loss: 0.028984 \n",
      "Step:  181   |   Loss: 0.018666 \n",
      "Step:  182   |   Loss: 0.023105 \n",
      "Step:  183   |   Loss: 0.011881 \n",
      "Step:  184   |   Loss: 0.009855 \n",
      "Step:  185   |   Loss: 0.011438 \n",
      "Step:  186   |   Loss: 0.010595 \n",
      "Step:  187   |   Loss: 0.013489 \n",
      "Step:  188   |   Loss: 0.002605 \n",
      "Step:  189   |   Loss: 0.006436 \n",
      "Step:  191   |   Loss: 0.006332 \n",
      "Step:  192   |   Loss: 0.010126 \n",
      "Step:  193   |   Loss: 0.005853 \n",
      "Step:  194   |   Loss: 0.005495 \n",
      "Step:  195   |   Loss: 0.023168 \n",
      "Step:  196   |   Loss: 0.007367 \n",
      "Step:  197   |   Loss: 0.008571 \n",
      "Step:  198   |   Loss: 0.012593 \n",
      "Step:  199   |   Loss: 0.002781 \n",
      "Step:  201   |   Loss: 0.006380 \n",
      "Step:  202   |   Loss: 0.009747 \n",
      "Step:  203   |   Loss: 0.006249 \n",
      "Step:  204   |   Loss: 0.007749 \n",
      "Step:  205   |   Loss: 0.008002 \n",
      "Step:  206   |   Loss: 0.012297 \n",
      "Step:  207   |   Loss: 0.040586 \n",
      "Step:  208   |   Loss: 0.005997 \n",
      "Step:  209   |   Loss: 0.015604 \n",
      "Step:  211   |   Loss: 0.002067 \n",
      "Step:  212   |   Loss: 0.017854 \n",
      "Step:  213   |   Loss: 0.012221 \n",
      "Step:  214   |   Loss: 0.004006 \n",
      "Step:  215   |   Loss: 0.011367 \n",
      "Step:  216   |   Loss: 0.006354 \n",
      "Step:  217   |   Loss: 0.002980 \n",
      "Step:  218   |   Loss: 0.014238 \n",
      "Step:  219   |   Loss: 0.005390 \n",
      "Step:  221   |   Loss: 0.035839 \n",
      "Step:  222   |   Loss: 0.007654 \n",
      "Step:  223   |   Loss: 0.016451 \n",
      "Step:  224   |   Loss: 0.008696 \n",
      "Step:  225   |   Loss: 0.003870 \n",
      "Step:  226   |   Loss: 0.020350 \n",
      "Step:  227   |   Loss: 0.003416 \n",
      "Step:  228   |   Loss: 0.007012 \n",
      "Step:  229   |   Loss: 0.008612 \n",
      "Step:  231   |   Loss: 0.015704 \n",
      "Step:  232   |   Loss: 0.002461 \n",
      "Step:  233   |   Loss: 0.001526 \n",
      "Step:  234   |   Loss: 0.025025 \n",
      "Step:  235   |   Loss: 0.000632 \n",
      "Step:  236   |   Loss: 0.018099 \n",
      "Step:  237   |   Loss: 0.002606 \n",
      "Step:  238   |   Loss: 0.003161 \n",
      "Step:  239   |   Loss: 0.009908 \n",
      "Step:  241   |   Loss: 0.009574 \n",
      "Step:  242   |   Loss: 0.007465 \n",
      "Step:  243   |   Loss: 0.002881 \n",
      "Step:  244   |   Loss: 0.009995 \n",
      "Step:  245   |   Loss: 0.010588 \n",
      "Step:  246   |   Loss: 0.003355 \n",
      "Step:  247   |   Loss: 0.012309 \n",
      "Step:  248   |   Loss: 0.007923 \n",
      "Step:  249   |   Loss: 0.006832 \n",
      "Step:  251   |   Loss: 0.003976 \n",
      "Step:  252   |   Loss: 0.002768 \n",
      "Step:  253   |   Loss: 0.013529 \n",
      "Step:  254   |   Loss: 0.003205 \n",
      "Step:  255   |   Loss: 0.002587 \n",
      "Step:  256   |   Loss: 0.007076 \n",
      "Step:  257   |   Loss: 0.004642 \n",
      "Step:  258   |   Loss: 0.005829 \n",
      "Step:  259   |   Loss: 0.021840 \n",
      "Step:  261   |   Loss: 0.011055 \n",
      "Step:  262   |   Loss: 0.023446 \n",
      "Step:  263   |   Loss: 0.001750 \n",
      "Step:  264   |   Loss: 0.005611 \n",
      "Step:  265   |   Loss: 0.006504 \n",
      "Step:  266   |   Loss: 0.005360 \n",
      "Step:  267   |   Loss: 0.011982 \n",
      "Step:  268   |   Loss: 0.001493 \n",
      "Step:  269   |   Loss: 0.005799 \n",
      "Step:  271   |   Loss: 0.002426 \n",
      "Step:  272   |   Loss: 0.005356 \n",
      "Step:  273   |   Loss: 0.017408 \n",
      "Step:  274   |   Loss: 0.000763 \n",
      "Step:  275   |   Loss: 0.000914 \n",
      "Step:  276   |   Loss: 0.005028 \n",
      "Step:  277   |   Loss: 0.002469 \n",
      "Step:  278   |   Loss: 0.001301 \n",
      "Step:  279   |   Loss: 0.005594 \n",
      "Step:  281   |   Loss: 0.002016 \n",
      "Step:  282   |   Loss: 0.005585 \n",
      "Step:  283   |   Loss: 0.000662 \n",
      "Step:  284   |   Loss: 0.008101 \n",
      "Step:  285   |   Loss: 0.003420 \n",
      "Step:  286   |   Loss: 0.065883 \n",
      "Step:  287   |   Loss: 0.007952 \n",
      "Step:  288   |   Loss: 0.016090 \n",
      "Step:  289   |   Loss: 0.020845 \n",
      "Step:  291   |   Loss: 0.001783 \n",
      "Step:  292   |   Loss: 0.023598 \n",
      "Step:  293   |   Loss: 0.002667 \n",
      "Step:  294   |   Loss: 0.003580 \n",
      "Step:  295   |   Loss: 0.006359 \n",
      "Step:  296   |   Loss: 0.002899 \n",
      "Step:  297   |   Loss: 0.002550 \n",
      "Step:  298   |   Loss: 0.009570 \n",
      "Step:  299   |   Loss: 0.001812 \n",
      "Step:  301   |   Loss: 0.002707 \n",
      "Step:  302   |   Loss: 0.011572 \n",
      "Step:  303   |   Loss: 0.025430 \n",
      "Step:  304   |   Loss: 0.008664 \n",
      "Step:  305   |   Loss: 0.000870 \n",
      "Step:  306   |   Loss: 0.005683 \n",
      "Step:  307   |   Loss: 0.001888 \n",
      "Step:  308   |   Loss: 0.002750 \n",
      "Step:  309   |   Loss: 0.012848 \n",
      "Step:  311   |   Loss: 0.013187 \n",
      "Step:  312   |   Loss: 0.012860 \n",
      "Step:  313   |   Loss: 0.002086 \n",
      "Step:  314   |   Loss: 0.001535 \n",
      "Step:  315   |   Loss: 0.013694 \n",
      "Step:  316   |   Loss: 0.003429 \n",
      "Step:  317   |   Loss: 0.002651 \n",
      "Step:  318   |   Loss: 0.007243 \n",
      "Step:  319   |   Loss: 0.004263 \n",
      "Step:  321   |   Loss: 0.000972 \n",
      "Step:  322   |   Loss: 0.011330 \n",
      "Step:  323   |   Loss: 0.003812 \n",
      "Step:  324   |   Loss: 0.001234 \n",
      "Step:  325   |   Loss: 0.016413 \n",
      "Step:  326   |   Loss: 0.006711 \n",
      "Step:  327   |   Loss: 0.019798 \n",
      "Step:  328   |   Loss: 0.012163 \n",
      "Step:  329   |   Loss: 0.004659 \n",
      "Step:  331   |   Loss: 0.002228 \n",
      "Step:  332   |   Loss: 0.003816 \n",
      "Step:  333   |   Loss: 0.013352 \n",
      "Step:  334   |   Loss: 0.036067 \n",
      "Step:  335   |   Loss: 0.057324 \n",
      "Step:  336   |   Loss: 0.015800 \n",
      "Step:  337   |   Loss: 0.020261 \n",
      "Step:  338   |   Loss: 0.021582 \n",
      "Step:  339   |   Loss: 0.008347 \n",
      "Step:  341   |   Loss: 0.002178 \n",
      "Step:  342   |   Loss: 0.003861 \n",
      "Step:  343   |   Loss: 0.004139 \n",
      "Step:  344   |   Loss: 0.001104 \n",
      "Step:  345   |   Loss: 0.010303 \n",
      "Step:  346   |   Loss: 0.000134 \n",
      "Step:  347   |   Loss: 0.000768 \n",
      "Step:  348   |   Loss: 0.001099 \n",
      "Step:  349   |   Loss: 0.003045 \n",
      "Step:  351   |   Loss: 0.002285 \n",
      "Step:  352   |   Loss: 0.004155 \n",
      "Step:  353   |   Loss: 0.003278 \n",
      "Step:  354   |   Loss: 0.004353 \n",
      "Step:  355   |   Loss: 0.000783 \n",
      "Step:  356   |   Loss: 0.004827 \n",
      "Step:  357   |   Loss: 0.007170 \n",
      "Step:  358   |   Loss: 0.004048 \n",
      "Step:  359   |   Loss: 0.001830 \n",
      "Step:  361   |   Loss: 0.001795 \n",
      "Step:  362   |   Loss: 0.000509 \n",
      "Step:  363   |   Loss: 0.012834 \n",
      "Step:  364   |   Loss: 0.003914 \n",
      "Step:  365   |   Loss: 0.001889 \n",
      "Step:  366   |   Loss: 0.012932 \n",
      "Step:  367   |   Loss: 0.003188 \n",
      "Step:  368   |   Loss: 0.008500 \n",
      "Step:  369   |   Loss: 0.012165 \n",
      "Step:  371   |   Loss: 0.002052 \n",
      "Step:  372   |   Loss: 0.000507 \n",
      "Step:  373   |   Loss: 0.013926 \n",
      "Step:  374   |   Loss: 0.000673 \n",
      "Step:  375   |   Loss: 0.003315 \n",
      "Step:  376   |   Loss: 0.003572 \n",
      "Step:  377   |   Loss: 0.001774 \n",
      "Step:  378   |   Loss: 0.000028 \n",
      "Step:  379   |   Loss: 0.016247 \n",
      "Step:  381   |   Loss: 0.013088 \n",
      "Step:  382   |   Loss: 0.004865 \n",
      "Step:  383   |   Loss: 0.019136 \n",
      "Step:  384   |   Loss: 0.000247 \n",
      "Step:  385   |   Loss: 0.003446 \n",
      "Step:  386   |   Loss: 0.004325 \n",
      "Step:  387   |   Loss: 0.000531 \n",
      "Step:  388   |   Loss: 0.000087 \n",
      "Step:  389   |   Loss: 0.015089 \n",
      "Step:  391   |   Loss: 0.011141 \n",
      "Step:  392   |   Loss: 0.004517 \n",
      "Step:  393   |   Loss: 0.028584 \n",
      "Step:  394   |   Loss: 0.000165 \n",
      "Step:  395   |   Loss: 0.002352 \n",
      "Step:  396   |   Loss: 0.008154 \n",
      "Step:  397   |   Loss: 0.001886 \n",
      "Step:  398   |   Loss: 0.003306 \n",
      "Step:  399   |   Loss: 0.005702 \n",
      "Step:  401   |   Loss: 0.003352 \n",
      "Step:  402   |   Loss: 0.001110 \n",
      "Step:  403   |   Loss: 0.014748 \n",
      "Step:  404   |   Loss: 0.000992 \n",
      "Step:  405   |   Loss: 0.000802 \n",
      "Step:  406   |   Loss: 0.002889 \n",
      "Step:  407   |   Loss: 0.007899 \n",
      "Step:  408   |   Loss: 0.004783 \n",
      "Step:  409   |   Loss: 0.000251 \n",
      "Step:  411   |   Loss: 0.005882 \n",
      "Step:  412   |   Loss: 0.007198 \n",
      "Step:  413   |   Loss: 0.003768 \n",
      "Step:  414   |   Loss: 0.013779 \n",
      "Step:  415   |   Loss: 0.000770 \n",
      "Step:  416   |   Loss: 0.000360 \n",
      "Step:  417   |   Loss: 0.008123 \n",
      "Step:  418   |   Loss: 0.000639 \n",
      "Step:  419   |   Loss: 0.005938 \n",
      "Step:  421   |   Loss: 0.025486 \n",
      "Step:  422   |   Loss: 0.001181 \n",
      "Step:  423   |   Loss: 0.002049 \n",
      "Step:  424   |   Loss: 0.010624 \n",
      "Step:  425   |   Loss: 0.000390 \n",
      "Step:  426   |   Loss: 0.003490 \n",
      "Step:  427   |   Loss: 0.012275 \n",
      "Step:  428   |   Loss: 0.009201 \n",
      "Step:  429   |   Loss: 0.008736 \n",
      "Step:  431   |   Loss: 0.024537 \n",
      "Step:  432   |   Loss: 0.000756 \n",
      "Step:  433   |   Loss: 0.001330 \n",
      "Step:  434   |   Loss: 0.004389 \n",
      "Step:  435   |   Loss: 0.001163 \n",
      "Step:  436   |   Loss: 0.001721 \n",
      "Step:  437   |   Loss: 0.002531 \n",
      "Step:  438   |   Loss: 0.006981 \n",
      "Step:  439   |   Loss: 0.001158 \n",
      "Step:  441   |   Loss: 0.018613 \n",
      "Step:  442   |   Loss: 0.004188 \n",
      "Step:  443   |   Loss: 0.014708 \n",
      "Step:  444   |   Loss: 0.005328 \n",
      "Step:  445   |   Loss: 0.037815 \n",
      "Step:  446   |   Loss: 0.001552 \n",
      "Step:  447   |   Loss: 0.003149 \n",
      "Step:  448   |   Loss: 0.025232 \n",
      "Step:  449   |   Loss: 0.000929 \n",
      "Step:  451   |   Loss: 0.005697 \n",
      "Step:  452   |   Loss: 0.005708 \n",
      "Step:  453   |   Loss: 0.000185 \n",
      "Step:  454   |   Loss: 0.016707 \n",
      "Step:  455   |   Loss: 0.000683 \n",
      "Step:  456   |   Loss: 0.003946 \n",
      "Step:  457   |   Loss: 0.006760 \n",
      "Step:  458   |   Loss: 0.003582 \n",
      "Step:  459   |   Loss: 0.001077 \n",
      "Step:  461   |   Loss: 0.005345 \n",
      "Step:  462   |   Loss: 0.000679 \n",
      "Step:  463   |   Loss: 0.000395 \n",
      "Step:  464   |   Loss: 0.005938 \n",
      "Step:  465   |   Loss: 0.000702 \n",
      "Step:  466   |   Loss: 0.002237 \n",
      "Step:  467   |   Loss: 0.006060 \n",
      "Step:  468   |   Loss: 0.004047 \n",
      "Step:  469   |   Loss: 0.004605 \n",
      "Step:  471   |   Loss: 0.005589 \n",
      "Step:  472   |   Loss: 0.002713 \n",
      "Step:  473   |   Loss: 0.000007 \n",
      "Step:  474   |   Loss: 0.014011 \n",
      "Step:  475   |   Loss: 0.000896 \n",
      "Step:  476   |   Loss: 0.003092 \n",
      "Step:  477   |   Loss: 0.008006 \n",
      "Step:  478   |   Loss: 0.001556 \n",
      "Step:  479   |   Loss: 0.002538 \n",
      "Step:  481   |   Loss: 0.004443 \n",
      "Step:  482   |   Loss: 0.001191 \n",
      "Step:  483   |   Loss: 0.000921 \n",
      "Step:  484   |   Loss: 0.005987 \n",
      "Step:  485   |   Loss: 0.002484 \n",
      "Step:  486   |   Loss: 0.004587 \n",
      "Step:  487   |   Loss: 0.001554 \n",
      "Step:  488   |   Loss: 0.009663 \n",
      "Step:  489   |   Loss: 0.002603 \n",
      "Step:  491   |   Loss: 0.012477 \n",
      "Step:  492   |   Loss: 0.003872 \n",
      "Step:  493   |   Loss: 0.014974 \n",
      "Step:  494   |   Loss: 0.001501 \n",
      "Step:  495   |   Loss: 0.018452 \n",
      "Step:  496   |   Loss: 0.001906 \n",
      "Step:  497   |   Loss: 0.002872 \n",
      "Step:  498   |   Loss: 0.000439 \n",
      "Step:  499   |   Loss: 0.002255 \n"
     ]
    }
   ],
   "source": [
    "mv_net.train()\n",
    "\n",
    "# Training loop\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X),batch_size):\n",
    "        inpt = X[b:b+batch_size,:,:]\n",
    "        target = y[b:b+batch_size]\n",
    "        x_batch = torch.tensor(inpt,dtype=torch.float32)    \n",
    "        y_batch = torch.tensor(target,dtype=torch.float32)\n",
    "    \n",
    "        mv_net.init_hidden(x_batch.size(0))\n",
    "    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \n",
    "    #    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "        output = mv_net(x_batch) \n",
    "        loss = criterion(output, y_batch)  \n",
    "#         print('PREDICTED:\\n', output); print('REAL:\\n', y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        optimizer.zero_grad()\n",
    "        #loss_list.append(loss.item())\n",
    "    if t%10:\n",
    "        print(('Step: {:4}   |   Loss: {:.6f} ').format(t, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 5])\n",
      "tensor([0.5379])\n"
     ]
    }
   ],
   "source": [
    "b = 1\n",
    "inpt = X[-b-batch_size:-b,:,:]\n",
    "target = y[b:b+batch_size]\n",
    "x_batch = torch.tensor(inpt,dtype=torch.float32)    \n",
    "y_batch = torch.tensor(target,dtype=torch.float32)\n",
    "\n",
    "#    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \n",
    "#    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "mv_net.init_hidden(x_batch.size(0))\n",
    "\n",
    "print(x_batch.size())\n",
    "output = mv_net(torch.tensor(X[-1-batch_size:-1,:,:],dtype=torch.float32))[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(mv_net(torch.tensor(X[-1-batch_size:-1,:,:],dtype=torch.float32))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 0, 20), got [1, 3, 20]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-edcd0ae61d1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmv_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Real:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/good-night-ml/src/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# lstm_out(with batch_first = True) is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0m\u001b[1;32m    534\u001b[0m                                'Expected hidden[0] size {}, got {}')\n\u001b[1;32m    535\u001b[0m         self.check_hidden_size(hidden[1], expected_hidden_size,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    194\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 0, 20), got [1, 3, 20]"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(3+len(X)):\n",
    "            print('Predicted:', mv_net(torch.tensor(X[-i-batch_size:-i,:,:],dtype=torch.float32))[0])\n",
    "            print('Real:', y[-i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the predicted time to send the message in a file, so that the Daemon can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected time to go to sleep:  2020-11-17 01:17:20\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "# with torch.no_grad():\n",
    "p = mv_net.forward(torch.tensor(X[-batch_size-1:-1:,:],dtype=torch.float32))[0].detach().numpy()\n",
    "p_sec = int(p[0]*(max_hour+24-min_hour)*3600)\n",
    "prediction = now.replace(hour=min_hour, minute=0, second=0) + timedelta(seconds=p_sec)\n",
    "print('Expected time to go to sleep: ', prediction.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "\n",
    "'''Write the value on a text file to be read by the Daemon'''\n",
    "with open ('prediction.txt','w') as z:\n",
    "    z.write(prediction.strftime(\"%Y-%m-%d %H:%M:%S\\n\"))\n",
    "z.close()\n",
    "\n",
    "with open ('data/prediction_list.txt','a') as z:\n",
    "    z.write(prediction.strftime(\"%Y-%m-%d %H:%M:%S\\n\"))\n",
    "z.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
