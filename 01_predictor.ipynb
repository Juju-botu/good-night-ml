{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from datetime import date\n",
    "import datetime\n",
    "from datetime import timedelta  \n",
    "import csv\n",
    "import holidays # for importing the public holidays\n",
    "import re\n",
    "import torch\n",
    "from src.utils import *\n",
    "from src.data_miner import DataMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 5\n",
    "min_hour = 21 # Minimum hour for sleep detection\n",
    "max_hour = 5\n",
    "train_window = 3 # Sequence length\n",
    "local_holidays = holidays.Italy(prov='BO') # Get the holidays in Bologna, Italy :)\n",
    "train_episodes = 500\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "data_dir = \"data\"\n",
    "dataset = \"data/LastSeenDataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature extraction: we first extract the features given the time series data of Telegram accesses.\n",
    "- Supposition: last Telegram access in very similar to the time the person goes to sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Possible features to extract: \n",
    "1. Last seen time (arguably the most important)\n",
    "2. Wake up time\n",
    "3. Number of Telegram accesses during the previous day\n",
    "4. Day of the week\n",
    "5. Public holiday presence in the following day (using the holidays library)\n",
    "6. (time spent on Telegram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First day is holiday:  False\n"
     ]
    }
   ],
   "source": [
    "with open(dataset, newline='') as csvfile:\n",
    "    date_list = list(csv.reader(csvfile))\n",
    "\n",
    "date_list = convert_to_dates(date_list)\n",
    "\n",
    "'''Test data: search calendar for local holidays'''\n",
    "print(\"First day is holiday: \", date_list[0][0] in local_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6002, 0.5434, 0.4465, 0.5033, 0.4888, 0.5380, 0.5200, 0.6680, 0.1981,\n",
      "         0.5418, 0.5891, 0.5230, 0.5878, 0.2870, 0.1545, 0.3483, 0.1007, 0.6694,\n",
      "         0.5091, 0.4906, 0.6093, 0.6412, 0.8530, 0.3883, 0.5664, 0.7656],\n",
      "        [0.6667, 0.5991, 0.6653, 0.6445, 0.7801, 0.6894, 0.6742, 0.6647, 1.0048,\n",
      "         0.6278, 0.7105, 0.6988, 0.6384, 0.8407, 0.9146, 0.7862, 1.1783, 0.4033,\n",
      "         0.6723, 0.6988, 0.6034, 0.5998, 0.5354, 0.7193, 0.7006, 0.5691],\n",
      "        [1.0000, 0.0000, 0.1667, 0.3333, 0.5000, 0.6667, 0.8333, 1.0000, 0.0000,\n",
      "         0.1667, 0.3333, 0.5000, 0.6667, 0.8333, 1.0000, 0.0000, 0.1667, 0.3333,\n",
      "         0.5000, 0.6667, 0.8333, 1.0000, 0.0000, 0.1667, 0.3333, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1400, 0.1200, 0.0600, 0.0500, 0.0700, 0.1400, 0.0967, 0.1400, 0.0300,\n",
      "         0.2100, 0.3400, 0.1400, 0.2600, 0.2200, 0.0500, 0.1700, 0.0300, 0.3900,\n",
      "         0.3600, 0.2500, 0.2600, 0.3300, 0.1400, 0.0900, 0.1300, 0.2000]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "data_tensor =  DataMiner(date_list).to_tensor(verbose=False)\n",
    "print(data_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the training data is not much, we can insert some noise to augment it; this will also make the model less prone to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "\n",
    "\n",
    "# We use the \"last 3\" trend\n",
    "# Credits: https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/\n",
    "'''The sequence on which we have a prediction is the last train_window days'''\n",
    "X, y = create_inout_sequences(data_tensor, train_window)\n",
    "X = X.transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- Time series data, so possible idea(s):\n",
    "    - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44649306 0.66525465 0.16666667 0.         0.06      ]\n",
      " [0.50329864 0.64453703 0.33333334 0.         0.05      ]\n",
      " [0.48875    0.7801157  0.5        0.         0.07      ]]\n",
      "[0.48875]\n"
     ]
    }
   ],
   "source": [
    "n_features = num_features # this is number of parallel inputs\n",
    "n_timesteps = train_window # this is number of timesteps\n",
    "\n",
    "# convert dataset into input/output\n",
    "\n",
    "# create NN\n",
    "mv_net = LSTM(n_features,n_timesteps)\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-1)\n",
    "\n",
    "print(X[2])\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    1   |   Loss: 0.062825 \n",
      "Step:    2   |   Loss: 0.057742 \n",
      "Step:    3   |   Loss: 0.018226 \n",
      "Step:    4   |   Loss: 0.026921 \n",
      "Step:    5   |   Loss: 0.036091 \n",
      "Step:    6   |   Loss: 0.023738 \n",
      "Step:    7   |   Loss: 0.024063 \n",
      "Step:    8   |   Loss: 0.027750 \n",
      "Step:    9   |   Loss: 0.024572 \n",
      "Step:   11   |   Loss: 0.024550 \n",
      "Step:   12   |   Loss: 0.022695 \n",
      "Step:   13   |   Loss: 0.022636 \n",
      "Step:   14   |   Loss: 0.024614 \n",
      "Step:   15   |   Loss: 0.024019 \n",
      "Step:   16   |   Loss: 0.023168 \n",
      "Step:   17   |   Loss: 0.024635 \n",
      "Step:   18   |   Loss: 0.025013 \n",
      "Step:   19   |   Loss: 0.024960 \n",
      "Step:   21   |   Loss: 0.023465 \n",
      "Step:   22   |   Loss: 0.025328 \n",
      "Step:   23   |   Loss: 0.023933 \n",
      "Step:   24   |   Loss: 0.023644 \n",
      "Step:   25   |   Loss: 0.024425 \n",
      "Step:   26   |   Loss: 0.024487 \n",
      "Step:   27   |   Loss: 0.025275 \n",
      "Step:   28   |   Loss: 0.024140 \n",
      "Step:   29   |   Loss: 0.026127 \n",
      "Step:   31   |   Loss: 0.024486 \n",
      "Step:   32   |   Loss: 0.028556 \n",
      "Step:   33   |   Loss: 0.024434 \n",
      "Step:   34   |   Loss: 0.025651 \n",
      "Step:   35   |   Loss: 0.024676 \n",
      "Step:   36   |   Loss: 0.022830 \n",
      "Step:   37   |   Loss: 0.025452 \n",
      "Step:   38   |   Loss: 0.027521 \n",
      "Step:   39   |   Loss: 0.023418 \n",
      "Step:   41   |   Loss: 0.023315 \n",
      "Step:   42   |   Loss: 0.024901 \n",
      "Step:   43   |   Loss: 0.027084 \n",
      "Step:   44   |   Loss: 0.024421 \n",
      "Step:   45   |   Loss: 0.027050 \n",
      "Step:   46   |   Loss: 0.025706 \n",
      "Step:   47   |   Loss: 0.024407 \n",
      "Step:   48   |   Loss: 0.024400 \n",
      "Step:   49   |   Loss: 0.024746 \n",
      "Step:   51   |   Loss: 0.024029 \n",
      "Step:   52   |   Loss: 0.024921 \n",
      "Step:   53   |   Loss: 0.025758 \n",
      "Step:   54   |   Loss: 0.024030 \n",
      "Step:   55   |   Loss: 0.026380 \n",
      "Step:   56   |   Loss: 0.023830 \n",
      "Step:   57   |   Loss: 0.026437 \n",
      "Step:   58   |   Loss: 0.025451 \n",
      "Step:   59   |   Loss: 0.024811 \n",
      "Step:   61   |   Loss: 0.023573 \n",
      "Step:   62   |   Loss: 0.025610 \n",
      "Step:   63   |   Loss: 0.024771 \n",
      "Step:   64   |   Loss: 0.027218 \n",
      "Step:   65   |   Loss: 0.025628 \n",
      "Step:   66   |   Loss: 0.024684 \n",
      "Step:   67   |   Loss: 0.023903 \n",
      "Step:   68   |   Loss: 0.026051 \n",
      "Step:   69   |   Loss: 0.024511 \n",
      "Step:   71   |   Loss: 0.024029 \n",
      "Step:   72   |   Loss: 0.025051 \n",
      "Step:   73   |   Loss: 0.025065 \n",
      "Step:   74   |   Loss: 0.024176 \n",
      "Step:   75   |   Loss: 0.026387 \n",
      "Step:   76   |   Loss: 0.025139 \n",
      "Step:   77   |   Loss: 0.027050 \n",
      "Step:   78   |   Loss: 0.024781 \n",
      "Step:   79   |   Loss: 0.025991 \n",
      "Step:   81   |   Loss: 0.025874 \n",
      "Step:   82   |   Loss: 0.024603 \n",
      "Step:   83   |   Loss: 0.026535 \n",
      "Step:   84   |   Loss: 0.024221 \n",
      "Step:   85   |   Loss: 0.024897 \n",
      "Step:   86   |   Loss: 0.025840 \n",
      "Step:   87   |   Loss: 0.024542 \n",
      "Step:   88   |   Loss: 0.025714 \n",
      "Step:   89   |   Loss: 0.024111 \n",
      "Step:   91   |   Loss: 0.025250 \n",
      "Step:   92   |   Loss: 0.023031 \n",
      "Step:   93   |   Loss: 0.025067 \n",
      "Step:   94   |   Loss: 0.025464 \n",
      "Step:   95   |   Loss: 0.024815 \n",
      "Step:   96   |   Loss: 0.023665 \n",
      "Step:   97   |   Loss: 0.024633 \n",
      "Step:   98   |   Loss: 0.023878 \n",
      "Step:   99   |   Loss: 0.023962 \n",
      "Step:  101   |   Loss: 0.023795 \n",
      "Step:  102   |   Loss: 0.023202 \n",
      "Step:  103   |   Loss: 0.022838 \n",
      "Step:  104   |   Loss: 0.023309 \n",
      "Step:  105   |   Loss: 0.022516 \n",
      "Step:  106   |   Loss: 0.022922 \n",
      "Step:  107   |   Loss: 0.022725 \n",
      "Step:  108   |   Loss: 0.021473 \n",
      "Step:  109   |   Loss: 0.022606 \n",
      "Step:  111   |   Loss: 0.020990 \n",
      "Step:  112   |   Loss: 0.021085 \n",
      "Step:  113   |   Loss: 0.020515 \n",
      "Step:  114   |   Loss: 0.020129 \n",
      "Step:  115   |   Loss: 0.019547 \n",
      "Step:  116   |   Loss: 0.018828 \n",
      "Step:  117   |   Loss: 0.018997 \n",
      "Step:  118   |   Loss: 0.019291 \n",
      "Step:  119   |   Loss: 0.018445 \n",
      "Step:  121   |   Loss: 0.017530 \n",
      "Step:  122   |   Loss: 0.018308 \n",
      "Step:  123   |   Loss: 0.017607 \n",
      "Step:  124   |   Loss: 0.017502 \n",
      "Step:  125   |   Loss: 0.017334 \n",
      "Step:  126   |   Loss: 0.017317 \n",
      "Step:  127   |   Loss: 0.016696 \n",
      "Step:  128   |   Loss: 0.015027 \n",
      "Step:  129   |   Loss: 0.015396 \n",
      "Step:  131   |   Loss: 0.012556 \n",
      "Step:  132   |   Loss: 0.012234 \n",
      "Step:  133   |   Loss: 0.013151 \n",
      "Step:  134   |   Loss: 0.013283 \n",
      "Step:  135   |   Loss: 0.007488 \n",
      "Step:  136   |   Loss: 0.009008 \n",
      "Step:  137   |   Loss: 0.020577 \n",
      "Step:  138   |   Loss: 0.010072 \n",
      "Step:  139   |   Loss: 0.019447 \n",
      "Step:  141   |   Loss: 0.006728 \n",
      "Step:  142   |   Loss: 0.014759 \n",
      "Step:  143   |   Loss: 0.004419 \n",
      "Step:  144   |   Loss: 0.015962 \n",
      "Step:  145   |   Loss: 0.016918 \n",
      "Step:  146   |   Loss: 0.019117 \n",
      "Step:  147   |   Loss: 0.006587 \n",
      "Step:  148   |   Loss: 0.005901 \n",
      "Step:  149   |   Loss: 0.007433 \n",
      "Step:  151   |   Loss: 0.008720 \n",
      "Step:  152   |   Loss: 0.007334 \n",
      "Step:  153   |   Loss: 0.001870 \n",
      "Step:  154   |   Loss: 0.010548 \n",
      "Step:  155   |   Loss: 0.009786 \n",
      "Step:  156   |   Loss: 0.007658 \n",
      "Step:  157   |   Loss: 0.009176 \n",
      "Step:  158   |   Loss: 0.004672 \n",
      "Step:  159   |   Loss: 0.032007 \n",
      "Step:  161   |   Loss: 0.029213 \n",
      "Step:  162   |   Loss: 0.009266 \n",
      "Step:  163   |   Loss: 0.015896 \n",
      "Step:  164   |   Loss: 0.011999 \n",
      "Step:  165   |   Loss: 0.002331 \n",
      "Step:  166   |   Loss: 0.003431 \n",
      "Step:  167   |   Loss: 0.007710 \n",
      "Step:  168   |   Loss: 0.012721 \n",
      "Step:  169   |   Loss: 0.011139 \n",
      "Step:  171   |   Loss: 0.003200 \n",
      "Step:  172   |   Loss: 0.004730 \n",
      "Step:  173   |   Loss: 0.000617 \n",
      "Step:  174   |   Loss: 0.011417 \n",
      "Step:  175   |   Loss: 0.014167 \n",
      "Step:  176   |   Loss: 0.010366 \n",
      "Step:  177   |   Loss: 0.018604 \n",
      "Step:  178   |   Loss: 0.006069 \n",
      "Step:  179   |   Loss: 0.010875 \n",
      "Step:  181   |   Loss: 0.004891 \n",
      "Step:  182   |   Loss: 0.014477 \n",
      "Step:  183   |   Loss: 0.014014 \n",
      "Step:  184   |   Loss: 0.004753 \n",
      "Step:  185   |   Loss: 0.036771 \n",
      "Step:  186   |   Loss: 0.020490 \n",
      "Step:  187   |   Loss: 0.023705 \n",
      "Step:  188   |   Loss: 0.015788 \n",
      "Step:  189   |   Loss: 0.010815 \n",
      "Step:  191   |   Loss: 0.009448 \n",
      "Step:  192   |   Loss: 0.011724 \n",
      "Step:  193   |   Loss: 0.004569 \n",
      "Step:  194   |   Loss: 0.014290 \n",
      "Step:  195   |   Loss: 0.005125 \n",
      "Step:  196   |   Loss: 0.006314 \n",
      "Step:  197   |   Loss: 0.018079 \n",
      "Step:  198   |   Loss: 0.022715 \n",
      "Step:  199   |   Loss: 0.015536 \n",
      "Step:  201   |   Loss: 0.008776 \n",
      "Step:  202   |   Loss: 0.011037 \n",
      "Step:  203   |   Loss: 0.008259 \n",
      "Step:  204   |   Loss: 0.005789 \n",
      "Step:  205   |   Loss: 0.018052 \n",
      "Step:  206   |   Loss: 0.006737 \n",
      "Step:  207   |   Loss: 0.008152 \n",
      "Step:  208   |   Loss: 0.010897 \n",
      "Step:  209   |   Loss: 0.004128 \n",
      "Step:  211   |   Loss: 0.007269 \n",
      "Step:  212   |   Loss: 0.007427 \n",
      "Step:  213   |   Loss: 0.011273 \n",
      "Step:  214   |   Loss: 0.024862 \n",
      "Step:  215   |   Loss: 0.007949 \n",
      "Step:  216   |   Loss: 0.022785 \n",
      "Step:  217   |   Loss: 0.019988 \n",
      "Step:  218   |   Loss: 0.014173 \n",
      "Step:  219   |   Loss: 0.032279 \n",
      "Step:  221   |   Loss: 0.021710 \n",
      "Step:  222   |   Loss: 0.015216 \n",
      "Step:  223   |   Loss: 0.010907 \n",
      "Step:  224   |   Loss: 0.007518 \n",
      "Step:  225   |   Loss: 0.005851 \n",
      "Step:  226   |   Loss: 0.007001 \n",
      "Step:  227   |   Loss: 0.011799 \n",
      "Step:  228   |   Loss: 0.003235 \n",
      "Step:  229   |   Loss: 0.001143 \n",
      "Step:  231   |   Loss: 0.006982 \n",
      "Step:  232   |   Loss: 0.007402 \n",
      "Step:  233   |   Loss: 0.004768 \n",
      "Step:  234   |   Loss: 0.011342 \n",
      "Step:  235   |   Loss: 0.001582 \n",
      "Step:  236   |   Loss: 0.004869 \n",
      "Step:  237   |   Loss: 0.047054 \n",
      "Step:  238   |   Loss: 0.010987 \n",
      "Step:  239   |   Loss: 0.023322 \n",
      "Step:  241   |   Loss: 0.019168 \n",
      "Step:  242   |   Loss: 0.019479 \n",
      "Step:  243   |   Loss: 0.016870 \n",
      "Step:  244   |   Loss: 0.017270 \n",
      "Step:  245   |   Loss: 0.018045 \n",
      "Step:  246   |   Loss: 0.013372 \n",
      "Step:  247   |   Loss: 0.007925 \n",
      "Step:  248   |   Loss: 0.006707 \n",
      "Step:  249   |   Loss: 0.002189 \n",
      "Step:  251   |   Loss: 0.002266 \n",
      "Step:  252   |   Loss: 0.000248 \n",
      "Step:  253   |   Loss: 0.001357 \n",
      "Step:  254   |   Loss: 0.005554 \n",
      "Step:  255   |   Loss: 0.002596 \n",
      "Step:  256   |   Loss: 0.002026 \n",
      "Step:  257   |   Loss: 0.000633 \n",
      "Step:  258   |   Loss: 0.001424 \n",
      "Step:  259   |   Loss: 0.000684 \n",
      "Step:  261   |   Loss: 0.001581 \n",
      "Step:  262   |   Loss: 0.004218 \n",
      "Step:  263   |   Loss: 0.005165 \n",
      "Step:  264   |   Loss: 0.002778 \n",
      "Step:  265   |   Loss: 0.003079 \n",
      "Step:  266   |   Loss: 0.024058 \n",
      "Step:  267   |   Loss: 0.008017 \n",
      "Step:  268   |   Loss: 0.002172 \n",
      "Step:  269   |   Loss: 0.007593 \n",
      "Step:  271   |   Loss: 0.000758 \n",
      "Step:  272   |   Loss: 0.006798 \n",
      "Step:  273   |   Loss: 0.000803 \n",
      "Step:  274   |   Loss: 0.007763 \n",
      "Step:  275   |   Loss: 0.007083 \n",
      "Step:  276   |   Loss: 0.020235 \n",
      "Step:  277   |   Loss: 0.002621 \n",
      "Step:  278   |   Loss: 0.003467 \n",
      "Step:  279   |   Loss: 0.035050 \n",
      "Step:  281   |   Loss: 0.018263 \n",
      "Step:  282   |   Loss: 0.017832 \n",
      "Step:  283   |   Loss: 0.010243 \n",
      "Step:  284   |   Loss: 0.006534 \n",
      "Step:  285   |   Loss: 0.002923 \n",
      "Step:  286   |   Loss: 0.000154 \n",
      "Step:  287   |   Loss: 0.000904 \n",
      "Step:  288   |   Loss: 0.002807 \n",
      "Step:  289   |   Loss: 0.000181 \n",
      "Step:  291   |   Loss: 0.009048 \n",
      "Step:  292   |   Loss: 0.006701 \n",
      "Step:  293   |   Loss: 0.003388 \n",
      "Step:  294   |   Loss: 0.001743 \n",
      "Step:  295   |   Loss: 0.007321 \n",
      "Step:  296   |   Loss: 0.008511 \n",
      "Step:  297   |   Loss: 0.021676 \n",
      "Step:  298   |   Loss: 0.004784 \n",
      "Step:  299   |   Loss: 0.007971 \n",
      "Step:  301   |   Loss: 0.002548 \n",
      "Step:  302   |   Loss: 0.003539 \n",
      "Step:  303   |   Loss: 0.013826 \n",
      "Step:  304   |   Loss: 0.003583 \n",
      "Step:  305   |   Loss: 0.001331 \n",
      "Step:  306   |   Loss: 0.002243 \n",
      "Step:  307   |   Loss: 0.007102 \n",
      "Step:  308   |   Loss: 0.002609 \n",
      "Step:  309   |   Loss: 0.002894 \n",
      "Step:  311   |   Loss: 0.028560 \n",
      "Step:  312   |   Loss: 0.006880 \n",
      "Step:  313   |   Loss: 0.006967 \n",
      "Step:  314   |   Loss: 0.007956 \n",
      "Step:  315   |   Loss: 0.002841 \n",
      "Step:  316   |   Loss: 0.003611 \n",
      "Step:  317   |   Loss: 0.025078 \n",
      "Step:  318   |   Loss: 0.002124 \n",
      "Step:  319   |   Loss: 0.006479 \n",
      "Step:  321   |   Loss: 0.003651 \n",
      "Step:  322   |   Loss: 0.003101 \n",
      "Step:  323   |   Loss: 0.010653 \n",
      "Step:  324   |   Loss: 0.001522 \n",
      "Step:  325   |   Loss: 0.000314 \n",
      "Step:  326   |   Loss: 0.001960 \n",
      "Step:  327   |   Loss: 0.000166 \n",
      "Step:  328   |   Loss: 0.002982 \n",
      "Step:  329   |   Loss: 0.000918 \n",
      "Step:  331   |   Loss: 0.001713 \n",
      "Step:  332   |   Loss: 0.011350 \n",
      "Step:  333   |   Loss: 0.002364 \n",
      "Step:  334   |   Loss: 0.015888 \n",
      "Step:  335   |   Loss: 0.004816 \n",
      "Step:  336   |   Loss: 0.013580 \n",
      "Step:  337   |   Loss: 0.008513 \n",
      "Step:  338   |   Loss: 0.005535 \n",
      "Step:  339   |   Loss: 0.010078 \n",
      "Step:  341   |   Loss: 0.003500 \n",
      "Step:  342   |   Loss: 0.007753 \n",
      "Step:  343   |   Loss: 0.002679 \n",
      "Step:  344   |   Loss: 0.003035 \n",
      "Step:  345   |   Loss: 0.002355 \n",
      "Step:  346   |   Loss: 0.001895 \n",
      "Step:  347   |   Loss: 0.002212 \n",
      "Step:  348   |   Loss: 0.007788 \n",
      "Step:  349   |   Loss: 0.001393 \n",
      "Step:  351   |   Loss: 0.005252 \n",
      "Step:  352   |   Loss: 0.000420 \n",
      "Step:  353   |   Loss: 0.004370 \n",
      "Step:  354   |   Loss: 0.002420 \n",
      "Step:  355   |   Loss: 0.005691 \n",
      "Step:  356   |   Loss: 0.004947 \n",
      "Step:  357   |   Loss: 0.004529 \n",
      "Step:  358   |   Loss: 0.005957 \n",
      "Step:  359   |   Loss: 0.003341 \n",
      "Step:  361   |   Loss: 0.008303 \n",
      "Step:  362   |   Loss: 0.007006 \n",
      "Step:  363   |   Loss: 0.040342 \n",
      "Step:  364   |   Loss: 0.009459 \n",
      "Step:  365   |   Loss: 0.002334 \n",
      "Step:  366   |   Loss: 0.036337 \n",
      "Step:  367   |   Loss: 0.010830 \n",
      "Step:  368   |   Loss: 0.020409 \n",
      "Step:  369   |   Loss: 0.008401 \n",
      "Step:  371   |   Loss: 0.008558 \n",
      "Step:  372   |   Loss: 0.002608 \n",
      "Step:  373   |   Loss: 0.004739 \n",
      "Step:  374   |   Loss: 0.002138 \n",
      "Step:  375   |   Loss: 0.000073 \n",
      "Step:  376   |   Loss: 0.004969 \n",
      "Step:  377   |   Loss: 0.001014 \n",
      "Step:  378   |   Loss: 0.003311 \n",
      "Step:  379   |   Loss: 0.002642 \n",
      "Step:  381   |   Loss: 0.003682 \n",
      "Step:  382   |   Loss: 0.001791 \n",
      "Step:  383   |   Loss: 0.005650 \n",
      "Step:  384   |   Loss: 0.004736 \n",
      "Step:  385   |   Loss: 0.005299 \n",
      "Step:  386   |   Loss: 0.001670 \n",
      "Step:  387   |   Loss: 0.006277 \n",
      "Step:  388   |   Loss: 0.002763 \n",
      "Step:  389   |   Loss: 0.001957 \n",
      "Step:  391   |   Loss: 0.001827 \n",
      "Step:  392   |   Loss: 0.001790 \n",
      "Step:  393   |   Loss: 0.003000 \n",
      "Step:  394   |   Loss: 0.002631 \n",
      "Step:  395   |   Loss: 0.009935 \n",
      "Step:  396   |   Loss: 0.001500 \n",
      "Step:  397   |   Loss: 0.005849 \n",
      "Step:  398   |   Loss: 0.002810 \n",
      "Step:  399   |   Loss: 0.004059 \n",
      "Step:  401   |   Loss: 0.012106 \n",
      "Step:  402   |   Loss: 0.001839 \n",
      "Step:  403   |   Loss: 0.008425 \n",
      "Step:  404   |   Loss: 0.007916 \n",
      "Step:  405   |   Loss: 0.008861 \n",
      "Step:  406   |   Loss: 0.005315 \n",
      "Step:  407   |   Loss: 0.002238 \n",
      "Step:  408   |   Loss: 0.007860 \n",
      "Step:  409   |   Loss: 0.007839 \n",
      "Step:  411   |   Loss: 0.001343 \n",
      "Step:  412   |   Loss: 0.010490 \n",
      "Step:  413   |   Loss: 0.015747 \n",
      "Step:  414   |   Loss: 0.007470 \n",
      "Step:  415   |   Loss: 0.003024 \n",
      "Step:  416   |   Loss: 0.009642 \n",
      "Step:  417   |   Loss: 0.015481 \n",
      "Step:  418   |   Loss: 0.012123 \n",
      "Step:  419   |   Loss: 0.017019 \n",
      "Step:  421   |   Loss: 0.005558 \n",
      "Step:  422   |   Loss: 0.018130 \n",
      "Step:  423   |   Loss: 0.005687 \n",
      "Step:  424   |   Loss: 0.007778 \n",
      "Step:  425   |   Loss: 0.003857 \n",
      "Step:  426   |   Loss: 0.002296 \n",
      "Step:  427   |   Loss: 0.005822 \n",
      "Step:  428   |   Loss: 0.011502 \n",
      "Step:  429   |   Loss: 0.014481 \n",
      "Step:  431   |   Loss: 0.004433 \n",
      "Step:  432   |   Loss: 0.015444 \n",
      "Step:  433   |   Loss: 0.001612 \n",
      "Step:  434   |   Loss: 0.005484 \n",
      "Step:  435   |   Loss: 0.015170 \n",
      "Step:  436   |   Loss: 0.001079 \n",
      "Step:  437   |   Loss: 0.000334 \n",
      "Step:  438   |   Loss: 0.003256 \n",
      "Step:  439   |   Loss: 0.001340 \n",
      "Step:  441   |   Loss: 0.006709 \n",
      "Step:  442   |   Loss: 0.003902 \n",
      "Step:  443   |   Loss: 0.007105 \n",
      "Step:  444   |   Loss: 0.007094 \n",
      "Step:  445   |   Loss: 0.004614 \n",
      "Step:  446   |   Loss: 0.001246 \n",
      "Step:  447   |   Loss: 0.002146 \n",
      "Step:  448   |   Loss: 0.004055 \n",
      "Step:  449   |   Loss: 0.003149 \n",
      "Step:  451   |   Loss: 0.003568 \n",
      "Step:  452   |   Loss: 0.000906 \n",
      "Step:  453   |   Loss: 0.000891 \n",
      "Step:  454   |   Loss: 0.003445 \n",
      "Step:  455   |   Loss: 0.000413 \n",
      "Step:  456   |   Loss: 0.001775 \n",
      "Step:  457   |   Loss: 0.000708 \n",
      "Step:  458   |   Loss: 0.005469 \n",
      "Step:  459   |   Loss: 0.005376 \n",
      "Step:  461   |   Loss: 0.009788 \n",
      "Step:  462   |   Loss: 0.002217 \n",
      "Step:  463   |   Loss: 0.000713 \n",
      "Step:  464   |   Loss: 0.002397 \n",
      "Step:  465   |   Loss: 0.004368 \n",
      "Step:  466   |   Loss: 0.002294 \n",
      "Step:  467   |   Loss: 0.002511 \n",
      "Step:  468   |   Loss: 0.004966 \n",
      "Step:  469   |   Loss: 0.000345 \n",
      "Step:  471   |   Loss: 0.001408 \n",
      "Step:  472   |   Loss: 0.000338 \n",
      "Step:  473   |   Loss: 0.001630 \n",
      "Step:  474   |   Loss: 0.000863 \n",
      "Step:  475   |   Loss: 0.001030 \n",
      "Step:  476   |   Loss: 0.001809 \n",
      "Step:  477   |   Loss: 0.000798 \n",
      "Step:  478   |   Loss: 0.000851 \n",
      "Step:  479   |   Loss: 0.002210 \n",
      "Step:  481   |   Loss: 0.002791 \n",
      "Step:  482   |   Loss: 0.004299 \n",
      "Step:  483   |   Loss: 0.004510 \n",
      "Step:  484   |   Loss: 0.009333 \n",
      "Step:  485   |   Loss: 0.008823 \n",
      "Step:  486   |   Loss: 0.009992 \n",
      "Step:  487   |   Loss: 0.009356 \n",
      "Step:  488   |   Loss: 0.047860 \n",
      "Step:  489   |   Loss: 0.004003 \n",
      "Step:  491   |   Loss: 0.034666 \n",
      "Step:  492   |   Loss: 0.008763 \n",
      "Step:  493   |   Loss: 0.006621 \n",
      "Step:  494   |   Loss: 0.006736 \n",
      "Step:  495   |   Loss: 0.003694 \n",
      "Step:  496   |   Loss: 0.013912 \n",
      "Step:  497   |   Loss: 0.003330 \n",
      "Step:  498   |   Loss: 0.003332 \n",
      "Step:  499   |   Loss: 0.011238 \n"
     ]
    }
   ],
   "source": [
    "mv_net.train()\n",
    "\n",
    "# Training loop\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X),batch_size):\n",
    "        inpt = X[b:b+batch_size,:,:]\n",
    "        target = y[b:b+batch_size]\n",
    "        x_batch = torch.tensor(inpt,dtype=torch.float32)    \n",
    "        y_batch = torch.tensor(target,dtype=torch.float32)\n",
    "    \n",
    "        mv_net.init_hidden(x_batch.size(0))\n",
    "    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \n",
    "    #    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "        output = mv_net(x_batch) \n",
    "        loss = criterion(output, y_batch)  \n",
    "#         print('PREDICTED:\\n', output); print('REAL:\\n', y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        optimizer.zero_grad()\n",
    "        #loss_list.append(loss.item())\n",
    "    if t%10:\n",
    "        print(('Step: {:4}   |   Loss: {:.6f} ').format(t, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 5])\n",
      "tensor([0.9221])\n"
     ]
    }
   ],
   "source": [
    "b = 1\n",
    "inpt = X[-b-batch_size:-b,:,:]\n",
    "target = y[b:b+batch_size]\n",
    "x_batch = torch.tensor(inpt,dtype=torch.float32)    \n",
    "y_batch = torch.tensor(target,dtype=torch.float32)\n",
    "\n",
    "#    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \n",
    "#    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "mv_net.init_hidden(x_batch.size(0))\n",
    "\n",
    "print(x_batch.size())\n",
    "output = mv_net(torch.tensor(X[-1-batch_size:-1,:,:],dtype=torch.float32))[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(mv_net(torch.tensor(X[-1-batch_size:-1,:,:],dtype=torch.float32))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([0.9443])\n",
      "Real: [0.765625]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('Predicted:', mv_net(torch.tensor(X[-1-batch_size:-1,:,:],dtype=torch.float32))[0])\n",
    "    print('Real:', y[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the predicted time to send the message in a file, so that the Daemon can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected time to go to sleep:  2020-11-16 04:33:17\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "# with torch.no_grad():\n",
    "p = mv_net.forward(torch.tensor(X[-batch_size-1:-1:,:],dtype=torch.float32))[0].detach().numpy()\n",
    "p_sec = int(p[0]*(max_hour+24-min_hour)*3600)\n",
    "prediction = now.replace(hour=min_hour, minute=0, second=0) + timedelta(seconds=p_sec)\n",
    "print('Expected time to go to sleep: ', prediction.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "\n",
    "'''Write the value on a text file to be read by the Daemon'''\n",
    "with open ('prediction.txt','w') as z:\n",
    "    z.write(prediction.strftime(\"%Y-%m-%d %H:%M:%S\\n\"))\n",
    "z.close()\n",
    "\n",
    "with open ('data/prediction_list.txt','a') as z:\n",
    "    z.write(prediction.strftime(\"%Y-%m-%d %H:%M:%S\\n\"))\n",
    "z.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
