{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from datetime import date\n",
    "import datetime\n",
    "from datetime import timedelta  \n",
    "import csv\n",
    "import holidays # for importing the public holidays\n",
    "import re\n",
    "import torch\n",
    "import sys; sys.path.append('../')\n",
    "from src.utils import *\n",
    "from src.data_miner import DataMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 5\n",
    "min_hour = 21 # Minimum hour for sleep detection\n",
    "max_hour = 5\n",
    "train_window = 3 # Sequence length\n",
    "local_holidays = holidays.Italy(prov='BO') # Get the holidays in Bologna, Italy :)\n",
    "train_episodes = 500\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "data_dir = \"data\"\n",
    "dataset = \"data/LastSeenDataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature extraction: we first extract the features given the time series data of Telegram accesses.\n",
    "- Supposition: last Telegram access in very similar to the time the person goes to sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Possible features to extract: \n",
    "1. Last seen time (arguably the most important)\n",
    "2. Wake up time\n",
    "3. Number of Telegram accesses during the previous day\n",
    "4. Day of the week\n",
    "5. Public holiday presence in the following day (using the holidays library)\n",
    "6. (time spent on Telegram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/LastSeenDataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fd692e70fa31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdate_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdate_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/LastSeenDataset.csv'"
     ]
    }
   ],
   "source": [
    "with open(dataset, newline='') as csvfile:\n",
    "    date_list = list(csv.reader(csvfile))\n",
    "\n",
    "date_list = convert_to_dates(date_list)\n",
    "\n",
    "'''Test data: search calendar for local holidays'''\n",
    "print(\"First day is holiday: \", date_list[0][0] in local_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor =  DataMiner(date_list).to_tensor(verbose=False)\n",
    "print(data_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the training data is not much, we can insert some noise to augment it; this will also make the model less prone to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "\n",
    "\n",
    "# We use the \"last 3\" trend\n",
    "# Credits: https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/\n",
    "'''The sequence on which we have a prediction is the last train_window days'''\n",
    "X, y = create_inout_sequences(data_tensor, train_window)\n",
    "X = X.transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- Time series data, so possible idea(s):\n",
    "    - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = num_features # this is number of parallel inputs\n",
    "n_timesteps = train_window # this is number of timesteps\n",
    "\n",
    "# convert dataset into input/output\n",
    "\n",
    "# create NN\n",
    "mv_net = LSTM(n_features,n_timesteps)\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-1)\n",
    "\n",
    "print(X[2])\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_net.train()\n",
    "\n",
    "# Training loop\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X),batch_size):\n",
    "        inpt = X[b:b+batch_size,:,:]\n",
    "        target = y[b:b+batch_size]\n",
    "        x_batch = torch.tensor(inpt,dtype=torch.float32)    \n",
    "        y_batch = torch.tensor(target,dtype=torch.float32)\n",
    "    \n",
    "        mv_net.init_hidden(x_batch.size(0))\n",
    "    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \n",
    "    #    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "        output = mv_net(x_batch) \n",
    "        loss = criterion(output, y_batch)  \n",
    "#         print('PREDICTED:\\n', output); print('REAL:\\n', y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        optimizer.zero_grad()\n",
    "        #loss_list.append(loss.item())\n",
    "    if t%10:\n",
    "        print(('Step: {:4}   |   Loss: {:.6f} ').format(t, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print('Predicted:', mv_net(torch.tensor(X[4:7,:,:],dtype=torch.float32))[0])\n",
    "    print('Real:', y[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the predicted time to send the message in a file, so that the Daemon can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "with torch.no_grad():\n",
    "    p = mv_net.forward(torch.tensor(X[-batch_size-2:-2:,:],dtype=torch.float32))[0].numpy()\n",
    "p_sec = int(p[0]*(max_hour+24-min_hour)*3600)\n",
    "prediction = now.replace(hour=min_hour, minute=0, second=0) + timedelta(seconds=p_sec)\n",
    "print('Expected time to go to sleep: ', prediction.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "\n",
    "'''Write the value on a text file to be read by the Daemon'''\n",
    "with open ('prediction.txt','w') as z:\n",
    "    z.write(prediction.strftime(\"%Y-%m-%d %H:%M:%S\\n\"))\n",
    "z.close()\n",
    "\n",
    "with open ('data/prediction_list.txt','a') as z:\n",
    "    z.write(prediction.strftime(\"%Y-%m-%d %H:%M:%S\\n\"))\n",
    "z.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
