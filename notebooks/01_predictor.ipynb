{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from datetime import date\n",
    "import datetime\n",
    "from datetime import timedelta  \n",
    "import csv\n",
    "import holidays # for importing the public holidays\n",
    "import re\n",
    "import torch\n",
    "from src.utils import *\n",
    "from src.data_miner import DataMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 5\n",
    "min_hour = 21 # Minimum hour for sleep detection\n",
    "max_hour = 5\n",
    "train_window = 3 # Sequence length\n",
    "local_holidays = holidays.Italy(prov='BO') # Get the holidays in Bologna, Italy :)\n",
    "train_episodes = 500\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "data_dir = \"data\"\n",
    "dataset = \"data/LastSeenDataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature extraction: we first extract the features given the time series data of Telegram accesses.\n",
    "- Supposition: last Telegram access in very similar to the time the person goes to sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Possible features to extract: \n",
    "1. Last seen time (arguably the most important)\n",
    "2. Wake up time\n",
    "3. Number of Telegram accesses during the previous day\n",
    "4. Day of the week\n",
    "5. Public holiday presence in the following day (using the holidays library)\n",
    "6. (time spent on Telegram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First day is holiday:  False\n"
     ]
    }
   ],
   "source": [
    "with open(dataset, newline='') as csvfile:\n",
    "    date_list = list(csv.reader(csvfile))\n",
    "\n",
    "date_list = convert_to_dates(date_list)\n",
    "\n",
    "'''Test data: search calendar for local holidays'''\n",
    "print(\"First day is holiday: \", date_list[0][0] in local_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6002, 0.5434, 0.4465, 0.5033, 0.4888, 0.5380, 0.5200, 0.6680, 0.1981,\n",
      "         0.5418, 0.5891, 0.5230, 0.5878, 0.2870, 0.1545, 0.3483, 0.1007, 0.6694,\n",
      "         0.5091, 0.4906, 0.6093],\n",
      "        [0.6667, 0.5991, 0.6653, 0.6445, 0.7801, 0.6894, 0.6742, 0.6647, 1.0048,\n",
      "         0.6278, 0.7105, 0.6988, 0.6384, 0.8407, 0.9146, 0.7862, 1.1783, 0.4033,\n",
      "         0.6723, 0.6988, 0.6034],\n",
      "        [1.0000, 0.0000, 0.1667, 0.3333, 0.5000, 0.6667, 0.8333, 1.0000, 0.0000,\n",
      "         0.1667, 0.3333, 0.5000, 0.6667, 0.8333, 1.0000, 0.0000, 0.1667, 0.3333,\n",
      "         0.5000, 0.6667, 0.8333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.1400, 0.1200, 0.0600, 0.0500, 0.0700, 0.1400, 0.0967, 0.1400, 0.0300,\n",
      "         0.2100, 0.3400, 0.1400, 0.2600, 0.2200, 0.0500, 0.1700, 0.0300, 0.3900,\n",
      "         0.3600, 0.2500, 0.2600]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "data_tensor =  DataMiner(date_list).to_tensor(verbose=False)\n",
    "print(data_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the training data is not much, we can insert some noise to augment it; this will also make the model less prone to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "\n",
    "\n",
    "# We use the \"last 3\" trend\n",
    "# Credits: https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/\n",
    "'''The sequence on which we have a prediction is the last train_window days'''\n",
    "X, y = create_inout_sequences(data_tensor, train_window)\n",
    "X = X.transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- Time series data, so possible idea(s):\n",
    "    - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44649306 0.66525465 0.16666667 0.         0.06      ]\n",
      " [0.50329864 0.64453703 0.33333334 0.         0.05      ]\n",
      " [0.48875    0.7801157  0.5        0.         0.07      ]]\n",
      "[0.48875]\n"
     ]
    }
   ],
   "source": [
    "n_features = num_features # this is number of parallel inputs\n",
    "n_timesteps = train_window # this is number of timesteps\n",
    "\n",
    "# convert dataset into input/output\n",
    "\n",
    "# create NN\n",
    "mv_net = LSTM(n_features,n_timesteps)\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-1)\n",
    "\n",
    "print(X[2])\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    1   |   Loss: 0.027416 \n",
      "Step:    2   |   Loss: 0.034268 \n",
      "Step:    3   |   Loss: 0.004917 \n",
      "Step:    4   |   Loss: 0.007367 \n",
      "Step:    5   |   Loss: 0.019113 \n",
      "Step:    6   |   Loss: 0.007467 \n",
      "Step:    7   |   Loss: 0.006685 \n",
      "Step:    8   |   Loss: 0.010298 \n",
      "Step:    9   |   Loss: 0.006568 \n",
      "Step:   11   |   Loss: 0.008106 \n",
      "Step:   12   |   Loss: 0.007904 \n",
      "Step:   13   |   Loss: 0.008229 \n",
      "Step:   14   |   Loss: 0.008929 \n",
      "Step:   15   |   Loss: 0.009317 \n",
      "Step:   16   |   Loss: 0.009705 \n",
      "Step:   17   |   Loss: 0.007661 \n",
      "Step:   18   |   Loss: 0.010211 \n",
      "Step:   19   |   Loss: 0.009548 \n",
      "Step:   21   |   Loss: 0.008490 \n",
      "Step:   22   |   Loss: 0.008237 \n",
      "Step:   23   |   Loss: 0.008862 \n",
      "Step:   24   |   Loss: 0.009592 \n",
      "Step:   25   |   Loss: 0.009798 \n",
      "Step:   26   |   Loss: 0.008494 \n",
      "Step:   27   |   Loss: 0.010324 \n",
      "Step:   28   |   Loss: 0.009068 \n",
      "Step:   29   |   Loss: 0.010798 \n",
      "Step:   31   |   Loss: 0.007674 \n",
      "Step:   32   |   Loss: 0.009610 \n",
      "Step:   33   |   Loss: 0.008762 \n",
      "Step:   34   |   Loss: 0.008820 \n",
      "Step:   35   |   Loss: 0.008992 \n",
      "Step:   36   |   Loss: 0.009941 \n",
      "Step:   37   |   Loss: 0.008910 \n",
      "Step:   38   |   Loss: 0.008650 \n",
      "Step:   39   |   Loss: 0.010674 \n",
      "Step:   41   |   Loss: 0.008205 \n",
      "Step:   42   |   Loss: 0.007977 \n",
      "Step:   43   |   Loss: 0.008222 \n",
      "Step:   44   |   Loss: 0.011770 \n",
      "Step:   45   |   Loss: 0.007593 \n",
      "Step:   46   |   Loss: 0.009271 \n",
      "Step:   47   |   Loss: 0.008058 \n",
      "Step:   48   |   Loss: 0.010784 \n",
      "Step:   49   |   Loss: 0.007046 \n",
      "Step:   51   |   Loss: 0.009194 \n",
      "Step:   52   |   Loss: 0.011518 \n",
      "Step:   53   |   Loss: 0.008395 \n",
      "Step:   54   |   Loss: 0.008484 \n",
      "Step:   55   |   Loss: 0.009375 \n",
      "Step:   56   |   Loss: 0.008500 \n",
      "Step:   57   |   Loss: 0.008912 \n",
      "Step:   58   |   Loss: 0.009296 \n",
      "Step:   59   |   Loss: 0.010410 \n",
      "Step:   61   |   Loss: 0.012080 \n",
      "Step:   62   |   Loss: 0.008907 \n",
      "Step:   63   |   Loss: 0.010140 \n",
      "Step:   64   |   Loss: 0.008528 \n",
      "Step:   65   |   Loss: 0.007782 \n",
      "Step:   66   |   Loss: 0.008368 \n",
      "Step:   67   |   Loss: 0.007813 \n",
      "Step:   68   |   Loss: 0.008125 \n",
      "Step:   69   |   Loss: 0.008661 \n",
      "Step:   71   |   Loss: 0.009250 \n",
      "Step:   72   |   Loss: 0.008031 \n",
      "Step:   73   |   Loss: 0.009788 \n",
      "Step:   74   |   Loss: 0.007021 \n",
      "Step:   75   |   Loss: 0.007713 \n",
      "Step:   76   |   Loss: 0.007965 \n",
      "Step:   77   |   Loss: 0.007186 \n",
      "Step:   78   |   Loss: 0.007738 \n",
      "Step:   79   |   Loss: 0.006499 \n",
      "Step:   81   |   Loss: 0.008229 \n",
      "Step:   82   |   Loss: 0.007113 \n",
      "Step:   83   |   Loss: 0.005896 \n",
      "Step:   84   |   Loss: 0.007330 \n",
      "Step:   85   |   Loss: 0.005521 \n",
      "Step:   86   |   Loss: 0.008083 \n",
      "Step:   87   |   Loss: 0.006307 \n",
      "Step:   88   |   Loss: 0.005503 \n",
      "Step:   89   |   Loss: 0.005777 \n",
      "Step:   91   |   Loss: 0.007808 \n",
      "Step:   92   |   Loss: 0.007571 \n",
      "Step:   93   |   Loss: 0.005656 \n",
      "Step:   94   |   Loss: 0.007357 \n",
      "Step:   95   |   Loss: 0.004412 \n",
      "Step:   96   |   Loss: 0.004165 \n",
      "Step:   97   |   Loss: 0.007280 \n",
      "Step:   98   |   Loss: 0.010949 \n",
      "Step:   99   |   Loss: 0.002752 \n",
      "Step:  101   |   Loss: 0.006313 \n",
      "Step:  102   |   Loss: 0.003575 \n",
      "Step:  103   |   Loss: 0.005994 \n",
      "Step:  104   |   Loss: 0.005715 \n",
      "Step:  105   |   Loss: 0.005527 \n",
      "Step:  106   |   Loss: 0.005506 \n",
      "Step:  107   |   Loss: 0.013820 \n",
      "Step:  108   |   Loss: 0.006513 \n",
      "Step:  109   |   Loss: 0.003089 \n",
      "Step:  111   |   Loss: 0.008495 \n",
      "Step:  112   |   Loss: 0.003252 \n",
      "Step:  113   |   Loss: 0.007098 \n",
      "Step:  114   |   Loss: 0.007060 \n",
      "Step:  115   |   Loss: 0.002467 \n",
      "Step:  116   |   Loss: 0.006144 \n",
      "Step:  117   |   Loss: 0.004254 \n",
      "Step:  118   |   Loss: 0.001740 \n",
      "Step:  119   |   Loss: 0.004195 \n",
      "Step:  121   |   Loss: 0.004321 \n",
      "Step:  122   |   Loss: 0.006087 \n",
      "Step:  123   |   Loss: 0.004681 \n",
      "Step:  124   |   Loss: 0.002021 \n",
      "Step:  125   |   Loss: 0.003674 \n",
      "Step:  126   |   Loss: 0.006919 \n",
      "Step:  127   |   Loss: 0.002930 \n",
      "Step:  128   |   Loss: 0.002969 \n",
      "Step:  129   |   Loss: 0.006179 \n",
      "Step:  131   |   Loss: 0.002780 \n",
      "Step:  132   |   Loss: 0.004068 \n",
      "Step:  133   |   Loss: 0.003594 \n",
      "Step:  134   |   Loss: 0.001750 \n",
      "Step:  135   |   Loss: 0.004496 \n",
      "Step:  136   |   Loss: 0.002360 \n",
      "Step:  137   |   Loss: 0.003785 \n",
      "Step:  138   |   Loss: 0.007348 \n",
      "Step:  139   |   Loss: 0.003685 \n",
      "Step:  141   |   Loss: 0.012551 \n",
      "Step:  142   |   Loss: 0.002622 \n",
      "Step:  143   |   Loss: 0.003164 \n",
      "Step:  144   |   Loss: 0.010423 \n",
      "Step:  145   |   Loss: 0.004604 \n",
      "Step:  146   |   Loss: 0.000262 \n",
      "Step:  147   |   Loss: 0.000738 \n",
      "Step:  148   |   Loss: 0.000507 \n",
      "Step:  149   |   Loss: 0.000865 \n",
      "Step:  151   |   Loss: 0.000584 \n",
      "Step:  152   |   Loss: 0.000433 \n",
      "Step:  153   |   Loss: 0.000602 \n",
      "Step:  154   |   Loss: 0.003124 \n",
      "Step:  155   |   Loss: 0.000325 \n",
      "Step:  156   |   Loss: 0.001352 \n",
      "Step:  157   |   Loss: 0.000200 \n",
      "Step:  158   |   Loss: 0.002877 \n",
      "Step:  159   |   Loss: 0.005510 \n",
      "Step:  161   |   Loss: 0.004429 \n",
      "Step:  162   |   Loss: 0.011686 \n",
      "Step:  163   |   Loss: 0.000983 \n",
      "Step:  164   |   Loss: 0.001358 \n",
      "Step:  165   |   Loss: 0.004288 \n",
      "Step:  166   |   Loss: 0.004119 \n",
      "Step:  167   |   Loss: 0.001397 \n",
      "Step:  168   |   Loss: 0.007984 \n",
      "Step:  169   |   Loss: 0.001963 \n",
      "Step:  171   |   Loss: 0.000939 \n",
      "Step:  172   |   Loss: 0.000234 \n",
      "Step:  173   |   Loss: 0.000444 \n",
      "Step:  174   |   Loss: 0.002408 \n",
      "Step:  175   |   Loss: 0.006513 \n",
      "Step:  176   |   Loss: 0.000531 \n",
      "Step:  177   |   Loss: 0.004159 \n",
      "Step:  178   |   Loss: 0.016624 \n",
      "Step:  179   |   Loss: 0.006977 \n",
      "Step:  181   |   Loss: 0.004479 \n",
      "Step:  182   |   Loss: 0.005164 \n",
      "Step:  183   |   Loss: 0.004862 \n",
      "Step:  184   |   Loss: 0.001754 \n",
      "Step:  185   |   Loss: 0.004790 \n",
      "Step:  186   |   Loss: 0.000445 \n",
      "Step:  187   |   Loss: 0.000396 \n",
      "Step:  188   |   Loss: 0.000333 \n",
      "Step:  189   |   Loss: 0.000794 \n",
      "Step:  191   |   Loss: 0.001064 \n",
      "Step:  192   |   Loss: 0.002023 \n",
      "Step:  193   |   Loss: 0.000180 \n",
      "Step:  194   |   Loss: 0.001515 \n",
      "Step:  195   |   Loss: 0.000982 \n",
      "Step:  196   |   Loss: 0.001509 \n",
      "Step:  197   |   Loss: 0.000735 \n",
      "Step:  198   |   Loss: 0.010634 \n",
      "Step:  199   |   Loss: 0.016858 \n",
      "Step:  201   |   Loss: 0.003519 \n",
      "Step:  202   |   Loss: 0.003131 \n",
      "Step:  203   |   Loss: 0.002079 \n",
      "Step:  204   |   Loss: 0.006558 \n",
      "Step:  205   |   Loss: 0.000875 \n",
      "Step:  206   |   Loss: 0.003291 \n",
      "Step:  207   |   Loss: 0.000469 \n",
      "Step:  208   |   Loss: 0.004951 \n",
      "Step:  209   |   Loss: 0.003709 \n",
      "Step:  211   |   Loss: 0.001790 \n",
      "Step:  212   |   Loss: 0.001065 \n",
      "Step:  213   |   Loss: 0.001456 \n",
      "Step:  214   |   Loss: 0.001346 \n",
      "Step:  215   |   Loss: 0.000650 \n",
      "Step:  216   |   Loss: 0.000409 \n",
      "Step:  217   |   Loss: 0.000679 \n",
      "Step:  218   |   Loss: 0.002192 \n",
      "Step:  219   |   Loss: 0.007332 \n",
      "Step:  221   |   Loss: 0.002416 \n",
      "Step:  222   |   Loss: 0.000054 \n",
      "Step:  223   |   Loss: 0.000605 \n",
      "Step:  224   |   Loss: 0.001066 \n",
      "Step:  225   |   Loss: 0.001951 \n",
      "Step:  226   |   Loss: 0.003887 \n",
      "Step:  227   |   Loss: 0.001685 \n",
      "Step:  228   |   Loss: 0.002430 \n",
      "Step:  229   |   Loss: 0.000407 \n",
      "Step:  231   |   Loss: 0.007897 \n",
      "Step:  232   |   Loss: 0.005002 \n",
      "Step:  233   |   Loss: 0.003465 \n",
      "Step:  234   |   Loss: 0.000211 \n",
      "Step:  235   |   Loss: 0.000392 \n",
      "Step:  236   |   Loss: 0.002180 \n",
      "Step:  237   |   Loss: 0.003894 \n",
      "Step:  238   |   Loss: 0.004058 \n",
      "Step:  239   |   Loss: 0.000841 \n",
      "Step:  241   |   Loss: 0.006183 \n",
      "Step:  242   |   Loss: 0.003351 \n",
      "Step:  243   |   Loss: 0.002516 \n",
      "Step:  244   |   Loss: 0.000121 \n",
      "Step:  245   |   Loss: 0.000294 \n",
      "Step:  246   |   Loss: 0.000549 \n",
      "Step:  247   |   Loss: 0.001032 \n",
      "Step:  248   |   Loss: 0.000756 \n",
      "Step:  249   |   Loss: 0.000187 \n",
      "Step:  251   |   Loss: 0.003777 \n",
      "Step:  252   |   Loss: 0.002352 \n",
      "Step:  253   |   Loss: 0.003678 \n",
      "Step:  254   |   Loss: 0.001347 \n",
      "Step:  255   |   Loss: 0.002093 \n",
      "Step:  256   |   Loss: 0.002912 \n",
      "Step:  257   |   Loss: 0.001211 \n",
      "Step:  258   |   Loss: 0.001073 \n",
      "Step:  259   |   Loss: 0.000062 \n",
      "Step:  261   |   Loss: 0.008181 \n",
      "Step:  262   |   Loss: 0.002912 \n",
      "Step:  263   |   Loss: 0.004918 \n",
      "Step:  264   |   Loss: 0.003529 \n",
      "Step:  265   |   Loss: 0.001847 \n",
      "Step:  266   |   Loss: 0.003772 \n",
      "Step:  267   |   Loss: 0.000300 \n",
      "Step:  268   |   Loss: 0.000382 \n",
      "Step:  269   |   Loss: 0.000228 \n",
      "Step:  271   |   Loss: 0.002959 \n",
      "Step:  272   |   Loss: 0.002668 \n",
      "Step:  273   |   Loss: 0.002877 \n",
      "Step:  274   |   Loss: 0.000477 \n",
      "Step:  275   |   Loss: 0.002266 \n",
      "Step:  276   |   Loss: 0.005135 \n",
      "Step:  277   |   Loss: 0.002143 \n",
      "Step:  278   |   Loss: 0.000493 \n",
      "Step:  279   |   Loss: 0.001611 \n",
      "Step:  281   |   Loss: 0.004419 \n",
      "Step:  282   |   Loss: 0.000812 \n",
      "Step:  283   |   Loss: 0.000281 \n",
      "Step:  284   |   Loss: 0.002396 \n",
      "Step:  285   |   Loss: 0.003780 \n",
      "Step:  286   |   Loss: 0.004624 \n",
      "Step:  287   |   Loss: 0.000300 \n",
      "Step:  288   |   Loss: 0.000949 \n",
      "Step:  289   |   Loss: 0.005040 \n",
      "Step:  291   |   Loss: 0.000764 \n",
      "Step:  292   |   Loss: 0.000409 \n",
      "Step:  293   |   Loss: 0.002974 \n",
      "Step:  294   |   Loss: 0.007260 \n",
      "Step:  295   |   Loss: 0.001581 \n",
      "Step:  296   |   Loss: 0.000527 \n",
      "Step:  297   |   Loss: 0.008154 \n",
      "Step:  298   |   Loss: 0.004556 \n",
      "Step:  299   |   Loss: 0.004985 \n",
      "Step:  301   |   Loss: 0.001159 \n",
      "Step:  302   |   Loss: 0.001555 \n",
      "Step:  303   |   Loss: 0.001180 \n",
      "Step:  304   |   Loss: 0.000593 \n",
      "Step:  305   |   Loss: 0.000208 \n",
      "Step:  306   |   Loss: 0.000656 \n",
      "Step:  307   |   Loss: 0.002534 \n",
      "Step:  308   |   Loss: 0.001328 \n",
      "Step:  309   |   Loss: 0.000318 \n",
      "Step:  311   |   Loss: 0.001773 \n",
      "Step:  312   |   Loss: 0.001495 \n",
      "Step:  313   |   Loss: 0.000292 \n",
      "Step:  314   |   Loss: 0.000837 \n",
      "Step:  315   |   Loss: 0.000922 \n",
      "Step:  316   |   Loss: 0.000484 \n",
      "Step:  317   |   Loss: 0.000213 \n",
      "Step:  318   |   Loss: 0.000567 \n",
      "Step:  319   |   Loss: 0.001077 \n",
      "Step:  321   |   Loss: 0.000024 \n",
      "Step:  322   |   Loss: 0.000324 \n",
      "Step:  323   |   Loss: 0.001246 \n",
      "Step:  324   |   Loss: 0.000803 \n",
      "Step:  325   |   Loss: 0.000055 \n",
      "Step:  326   |   Loss: 0.000212 \n",
      "Step:  327   |   Loss: 0.000703 \n",
      "Step:  328   |   Loss: 0.000418 \n",
      "Step:  329   |   Loss: 0.000226 \n",
      "Step:  331   |   Loss: 0.000666 \n",
      "Step:  332   |   Loss: 0.000065 \n",
      "Step:  333   |   Loss: 0.000584 \n",
      "Step:  334   |   Loss: 0.000880 \n",
      "Step:  335   |   Loss: 0.000393 \n",
      "Step:  336   |   Loss: 0.000064 \n",
      "Step:  337   |   Loss: 0.000249 \n",
      "Step:  338   |   Loss: 0.000467 \n",
      "Step:  339   |   Loss: 0.000013 \n",
      "Step:  341   |   Loss: 0.001333 \n",
      "Step:  342   |   Loss: 0.001200 \n",
      "Step:  343   |   Loss: 0.001413 \n",
      "Step:  344   |   Loss: 0.002821 \n",
      "Step:  345   |   Loss: 0.004225 \n",
      "Step:  346   |   Loss: 0.000067 \n",
      "Step:  347   |   Loss: 0.001184 \n",
      "Step:  348   |   Loss: 0.004387 \n",
      "Step:  349   |   Loss: 0.001344 \n",
      "Step:  351   |   Loss: 0.000253 \n",
      "Step:  352   |   Loss: 0.001350 \n",
      "Step:  353   |   Loss: 0.001466 \n",
      "Step:  354   |   Loss: 0.001225 \n",
      "Step:  355   |   Loss: 0.000483 \n",
      "Step:  356   |   Loss: 0.000822 \n",
      "Step:  357   |   Loss: 0.002843 \n",
      "Step:  358   |   Loss: 0.003914 \n",
      "Step:  359   |   Loss: 0.000176 \n",
      "Step:  361   |   Loss: 0.004714 \n",
      "Step:  362   |   Loss: 0.002566 \n",
      "Step:  363   |   Loss: 0.001310 \n",
      "Step:  364   |   Loss: 0.000784 \n",
      "Step:  365   |   Loss: 0.002872 \n",
      "Step:  366   |   Loss: 0.006068 \n",
      "Step:  367   |   Loss: 0.001083 \n",
      "Step:  368   |   Loss: 0.000229 \n",
      "Step:  369   |   Loss: 0.003292 \n",
      "Step:  371   |   Loss: 0.005602 \n",
      "Step:  372   |   Loss: 0.000046 \n",
      "Step:  373   |   Loss: 0.001181 \n",
      "Step:  374   |   Loss: 0.004671 \n",
      "Step:  375   |   Loss: 0.001664 \n",
      "Step:  376   |   Loss: 0.000623 \n",
      "Step:  377   |   Loss: 0.011764 \n",
      "Step:  378   |   Loss: 0.002045 \n",
      "Step:  379   |   Loss: 0.000661 \n",
      "Step:  381   |   Loss: 0.001808 \n",
      "Step:  382   |   Loss: 0.001019 \n",
      "Step:  383   |   Loss: 0.000339 \n",
      "Step:  384   |   Loss: 0.000566 \n",
      "Step:  385   |   Loss: 0.001425 \n",
      "Step:  386   |   Loss: 0.001851 \n",
      "Step:  387   |   Loss: 0.000383 \n",
      "Step:  388   |   Loss: 0.000475 \n",
      "Step:  389   |   Loss: 0.000247 \n",
      "Step:  391   |   Loss: 0.000659 \n",
      "Step:  392   |   Loss: 0.000757 \n",
      "Step:  393   |   Loss: 0.000040 \n",
      "Step:  394   |   Loss: 0.000080 \n",
      "Step:  395   |   Loss: 0.000474 \n",
      "Step:  396   |   Loss: 0.000385 \n",
      "Step:  397   |   Loss: 0.000075 \n",
      "Step:  398   |   Loss: 0.000216 \n",
      "Step:  399   |   Loss: 0.000326 \n",
      "Step:  401   |   Loss: 0.000072 \n",
      "Step:  402   |   Loss: 0.000682 \n",
      "Step:  403   |   Loss: 0.000465 \n",
      "Step:  404   |   Loss: 0.000094 \n",
      "Step:  405   |   Loss: 0.001453 \n",
      "Step:  406   |   Loss: 0.000804 \n",
      "Step:  407   |   Loss: 0.000117 \n",
      "Step:  408   |   Loss: 0.002823 \n",
      "Step:  409   |   Loss: 0.001441 \n",
      "Step:  411   |   Loss: 0.002874 \n",
      "Step:  412   |   Loss: 0.002444 \n",
      "Step:  413   |   Loss: 0.001543 \n",
      "Step:  414   |   Loss: 0.000888 \n",
      "Step:  415   |   Loss: 0.002729 \n",
      "Step:  416   |   Loss: 0.001129 \n",
      "Step:  417   |   Loss: 0.000044 \n",
      "Step:  418   |   Loss: 0.001476 \n",
      "Step:  419   |   Loss: 0.001291 \n",
      "Step:  421   |   Loss: 0.001126 \n",
      "Step:  422   |   Loss: 0.000487 \n",
      "Step:  423   |   Loss: 0.000005 \n",
      "Step:  424   |   Loss: 0.001214 \n",
      "Step:  425   |   Loss: 0.000953 \n",
      "Step:  426   |   Loss: 0.000025 \n",
      "Step:  427   |   Loss: 0.000555 \n",
      "Step:  428   |   Loss: 0.000891 \n",
      "Step:  429   |   Loss: 0.000025 \n",
      "Step:  431   |   Loss: 0.002977 \n",
      "Step:  432   |   Loss: 0.000214 \n",
      "Step:  433   |   Loss: 0.000483 \n",
      "Step:  434   |   Loss: 0.005494 \n",
      "Step:  435   |   Loss: 0.000825 \n",
      "Step:  436   |   Loss: 0.001646 \n",
      "Step:  437   |   Loss: 0.008678 \n",
      "Step:  438   |   Loss: 0.000819 \n",
      "Step:  439   |   Loss: 0.000367 \n",
      "Step:  441   |   Loss: 0.002777 \n",
      "Step:  442   |   Loss: 0.000027 \n",
      "Step:  443   |   Loss: 0.000430 \n",
      "Step:  444   |   Loss: 0.000909 \n",
      "Step:  445   |   Loss: 0.000066 \n",
      "Step:  446   |   Loss: 0.001198 \n",
      "Step:  447   |   Loss: 0.001775 \n",
      "Step:  448   |   Loss: 0.000058 \n",
      "Step:  449   |   Loss: 0.000302 \n",
      "Step:  451   |   Loss: 0.000390 \n",
      "Step:  452   |   Loss: 0.000264 \n",
      "Step:  453   |   Loss: 0.001606 \n",
      "Step:  454   |   Loss: 0.000721 \n",
      "Step:  455   |   Loss: 0.000010 \n",
      "Step:  456   |   Loss: 0.001201 \n",
      "Step:  457   |   Loss: 0.001115 \n",
      "Step:  458   |   Loss: 0.000094 \n",
      "Step:  459   |   Loss: 0.000670 \n",
      "Step:  461   |   Loss: 0.000791 \n",
      "Step:  462   |   Loss: 0.001367 \n",
      "Step:  463   |   Loss: 0.003336 \n",
      "Step:  464   |   Loss: 0.000147 \n",
      "Step:  465   |   Loss: 0.001321 \n",
      "Step:  466   |   Loss: 0.003886 \n",
      "Step:  467   |   Loss: 0.000353 \n",
      "Step:  468   |   Loss: 0.001365 \n",
      "Step:  469   |   Loss: 0.002640 \n",
      "Step:  471   |   Loss: 0.001109 \n",
      "Step:  472   |   Loss: 0.004401 \n",
      "Step:  473   |   Loss: 0.000553 \n",
      "Step:  474   |   Loss: 0.000559 \n",
      "Step:  475   |   Loss: 0.003655 \n",
      "Step:  476   |   Loss: 0.000863 \n",
      "Step:  477   |   Loss: 0.000917 \n",
      "Step:  478   |   Loss: 0.005257 \n",
      "Step:  479   |   Loss: 0.001009 \n",
      "Step:  481   |   Loss: 0.006076 \n",
      "Step:  482   |   Loss: 0.002439 \n",
      "Step:  483   |   Loss: 0.000227 \n",
      "Step:  484   |   Loss: 0.003361 \n",
      "Step:  485   |   Loss: 0.001306 \n",
      "Step:  486   |   Loss: 0.000222 \n",
      "Step:  487   |   Loss: 0.002279 \n",
      "Step:  488   |   Loss: 0.001014 \n",
      "Step:  489   |   Loss: 0.000217 \n",
      "Step:  491   |   Loss: 0.000911 \n",
      "Step:  492   |   Loss: 0.000041 \n",
      "Step:  493   |   Loss: 0.001638 \n",
      "Step:  494   |   Loss: 0.001491 \n",
      "Step:  495   |   Loss: 0.000026 \n",
      "Step:  496   |   Loss: 0.000748 \n",
      "Step:  497   |   Loss: 0.001022 \n",
      "Step:  498   |   Loss: 0.000074 \n",
      "Step:  499   |   Loss: 0.001495 \n"
     ]
    }
   ],
   "source": [
    "mv_net.train()\n",
    "\n",
    "# Training loop\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X),batch_size):\n",
    "        inpt = X[b:b+batch_size,:,:]\n",
    "        target = y[b:b+batch_size]\n",
    "        x_batch = torch.tensor(inpt,dtype=torch.float32)    \n",
    "        y_batch = torch.tensor(target,dtype=torch.float32)\n",
    "    \n",
    "        mv_net.init_hidden(x_batch.size(0))\n",
    "    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \n",
    "    #    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "        output = mv_net(x_batch) \n",
    "        loss = criterion(output, y_batch)  \n",
    "#         print('PREDICTED:\\n', output); print('REAL:\\n', y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        optimizer.zero_grad()\n",
    "        #loss_list.append(loss.item())\n",
    "    if t%10:\n",
    "        print(('Step: {:4}   |   Loss: {:.6f} ').format(t, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([0.6581])\n",
      "Real: [0.66802083]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('Predicted:', mv_net(torch.tensor(X[4:7,:,:],dtype=torch.float32))[0])\n",
    "    print('Real:', y[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the predicted time to send the message in a file, so that the Daemon can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected time to go to sleep:  2020-11-12 22:59:20\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "with torch.no_grad():\n",
    "    p = mv_net.forward(torch.tensor(X[-batch_size-2:-2:,:],dtype=torch.float32))[0].numpy()\n",
    "p_sec = int(p[0]*(max_hour+24-min_hour)*3600)\n",
    "prediction = now.replace(hour=min_hour, minute=0, second=0) + timedelta(seconds=p_sec)\n",
    "print('Expected time to go to sleep: ', prediction.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "\n",
    "'''Write the value on a text file to be read by the Daemon'''\n",
    "with open ('prediction.txt','w') as z:\n",
    "    z.write(prediction.strftime(\"%Y-%m-%d %H:%M:%S\\n\"))\n",
    "z.close()\n",
    "\n",
    "with open ('data/prediction_list.txt','a') as z:\n",
    "    z.write(prediction.strftime(\"%Y-%m-%d %H:%M:%S\\n\"))\n",
    "z.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
