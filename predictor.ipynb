{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/fedebotu/Documents/good-night-ml\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from datetime import date\n",
    "import datetime\n",
    "from datetime import timedelta  \n",
    "import csv\n",
    "import holidays # for importing the public holidays\n",
    "import re\n",
    "import torch\n",
    "from src.utils import *\n",
    "from src.data_miner import DataMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 5\n",
    "min_hour = 21 # Minimum hour for sleep detection\n",
    "max_hour = 5\n",
    "train_window = 3 # Sequence length\n",
    "local_holidays = holidays.Italy(prov='BO') # Get the holidays in Bologna, Italy :)\n",
    "train_episodes = 500\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "data_dir = \"data\"\n",
    "dataset = \"data/LastSeenDataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature extraction: we first extract the features given the time series data of Telegram accesses.\n",
    "- Supposition: last Telegram access in very similar to the time the person goes to sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Possible features to extract: \n",
    "1. Last seen time (arguably the most important)\n",
    "2. Wake up time\n",
    "3. Number of Telegram accesses during the previous day\n",
    "4. Day of the week\n",
    "5. Public holiday presence in the following day (using the holidays library)\n",
    "6. (time spent on Telegram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First day is holiday:  False\n"
     ]
    }
   ],
   "source": [
    "with open(dataset, newline='') as csvfile:\n",
    "    date_list = list(csv.reader(csvfile))\n",
    "\n",
    "date_list = convert_to_dates(date_list)\n",
    "\n",
    "'''Test data: search calendar for local holidays'''\n",
    "print(\"First day is holiday: \", date_list[0][0] in local_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6002, 0.5434, 0.4465, 0.5033, 0.4888, 0.5380, 0.5200, 0.6680, 0.1981,\n",
      "         0.5418, 0.5891, 0.5230, 0.5878, 0.2870, 0.1545, 0.3483, 0.1007, 0.6694,\n",
      "         0.5091, 0.4906, 0.6093],\n",
      "        [0.6667, 0.5991, 0.6653, 0.6445, 0.7801, 0.6894, 0.6742, 0.6647, 1.0048,\n",
      "         0.6278, 0.7105, 0.6988, 0.6384, 0.8407, 0.9146, 0.7862, 1.1783, 0.4033,\n",
      "         0.6723, 0.6988, 0.6034],\n",
      "        [1.0000, 0.0000, 0.1667, 0.3333, 0.5000, 0.6667, 0.8333, 1.0000, 0.0000,\n",
      "         0.1667, 0.3333, 0.5000, 0.6667, 0.8333, 1.0000, 0.0000, 0.1667, 0.3333,\n",
      "         0.5000, 0.6667, 0.8333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.1400, 0.1200, 0.0600, 0.0500, 0.0700, 0.1400, 0.0967, 0.1400, 0.0300,\n",
      "         0.2100, 0.3400, 0.1400, 0.2600, 0.2200, 0.0500, 0.1700, 0.0300, 0.3900,\n",
      "         0.3600, 0.2500, 0.2600]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "data_tensor =  DataMiner(date_list).to_tensor(verbose=False)\n",
    "print(data_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the training data is not much, we can insert some noise to augment it; this will also make the model less prone to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "\n",
    "\n",
    "# We use the \"last 3\" trend\n",
    "# Credits: https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/\n",
    "'''The sequence on which we have a prediction is the last train_window days'''\n",
    "X, y = create_inout_sequences(data_tensor, train_window)\n",
    "X = X.transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- Time series data, so possible idea(s):\n",
    "    - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    '''We use a model which should predict time series data (e.g. RNN, LSTM, Transformer)'''\n",
    "    def __init__(self,n_features,seq_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_length\n",
    "        self.n_hidden = 20 # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "    \n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features, \n",
    "                                 hidden_size = self.n_hidden,\n",
    "                                 num_layers = self.n_layers, \n",
    "                                 batch_first = True)\n",
    "        # according to pytorch docs LSTM output is \n",
    "        # (batch_size,seq_len, num_directions * hidden_size)\n",
    "        # when considering batch_first = True\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden*self.seq_len, 1)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # even with batch_first = True this remains same as docs\n",
    "        hidden_state = torch.rand(self.n_layers,batch_size,self.n_hidden)\n",
    "        cell_state = torch.rand(self.n_layers,batch_size,self.n_hidden)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):        \n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        lstm_out, self.hidden = self.l_lstm(x,self.hidden)\n",
    "        # lstm_out(with batch_first = True) is \n",
    "        # (batch_size,seq_len,num_directions * hidden_size)\n",
    "        # for following linear layer we want to keep batch_size dimension and merge rest       \n",
    "        # .contiguous() -> solves tensor compatibility error\n",
    "        x = lstm_out.contiguous().view(batch_size,-1)\n",
    "        return self.l_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44649306 0.66525465 0.16666667 0.         0.06      ]\n",
      " [0.50329864 0.64453703 0.33333334 0.         0.05      ]\n",
      " [0.48875    0.7801157  0.5        0.         0.07      ]]\n",
      "[0.48875]\n"
     ]
    }
   ],
   "source": [
    "n_features = num_features # this is number of parallel inputs\n",
    "n_timesteps = train_window # this is number of timesteps\n",
    "\n",
    "# convert dataset into input/output\n",
    "\n",
    "# create NN\n",
    "mv_net = LSTM(n_features,n_timesteps)\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-1)\n",
    "\n",
    "print(X[2])\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    1   |   Loss: 0.000329 \n",
      "Step:    2   |   Loss: 0.000173 \n",
      "Step:    3   |   Loss: 0.000079 \n",
      "Step:    4   |   Loss: 0.000006 \n",
      "Step:    5   |   Loss: 0.000166 \n",
      "Step:    6   |   Loss: 0.000427 \n",
      "Step:    7   |   Loss: 0.002964 \n",
      "Step:    8   |   Loss: 0.002892 \n",
      "Step:    9   |   Loss: 0.002877 \n",
      "Step:   11   |   Loss: 0.003399 \n",
      "Step:   12   |   Loss: 0.008009 \n",
      "Step:   13   |   Loss: 0.001821 \n",
      "Step:   14   |   Loss: 0.000750 \n",
      "Step:   15   |   Loss: 0.001225 \n",
      "Step:   16   |   Loss: 0.004458 \n",
      "Step:   17   |   Loss: 0.004392 \n",
      "Step:   18   |   Loss: 0.000040 \n",
      "Step:   19   |   Loss: 0.000182 \n",
      "Step:   21   |   Loss: 0.000378 \n",
      "Step:   22   |   Loss: 0.000301 \n",
      "Step:   23   |   Loss: 0.000828 \n",
      "Step:   24   |   Loss: 0.000440 \n",
      "Step:   25   |   Loss: 0.000046 \n",
      "Step:   26   |   Loss: 0.000556 \n",
      "Step:   27   |   Loss: 0.000590 \n",
      "Step:   28   |   Loss: 0.000189 \n",
      "Step:   29   |   Loss: 0.003391 \n",
      "Step:   31   |   Loss: 0.006830 \n",
      "Step:   32   |   Loss: 0.000561 \n",
      "Step:   33   |   Loss: 0.003516 \n",
      "Step:   34   |   Loss: 0.008054 \n",
      "Step:   35   |   Loss: 0.001039 \n",
      "Step:   36   |   Loss: 0.000021 \n",
      "Step:   37   |   Loss: 0.003736 \n",
      "Step:   38   |   Loss: 0.007690 \n",
      "Step:   39   |   Loss: 0.008190 \n",
      "Step:   41   |   Loss: 0.005029 \n",
      "Step:   42   |   Loss: 0.007099 \n",
      "Step:   43   |   Loss: 0.000136 \n",
      "Step:   44   |   Loss: 0.000327 \n",
      "Step:   45   |   Loss: 0.006476 \n",
      "Step:   46   |   Loss: 0.006604 \n",
      "Step:   47   |   Loss: 0.003267 \n",
      "Step:   48   |   Loss: 0.003342 \n",
      "Step:   49   |   Loss: 0.010399 \n",
      "Step:   51   |   Loss: 0.000436 \n",
      "Step:   52   |   Loss: 0.001881 \n",
      "Step:   53   |   Loss: 0.005863 \n",
      "Step:   54   |   Loss: 0.002420 \n",
      "Step:   55   |   Loss: 0.001070 \n",
      "Step:   56   |   Loss: 0.002283 \n",
      "Step:   57   |   Loss: 0.006953 \n",
      "Step:   58   |   Loss: 0.005252 \n",
      "Step:   59   |   Loss: 0.000162 \n",
      "Step:   61   |   Loss: 0.004198 \n",
      "Step:   62   |   Loss: 0.001649 \n",
      "Step:   63   |   Loss: 0.000362 \n",
      "Step:   64   |   Loss: 0.001628 \n",
      "Step:   65   |   Loss: 0.004653 \n",
      "Step:   66   |   Loss: 0.003268 \n",
      "Step:   67   |   Loss: 0.000066 \n",
      "Step:   68   |   Loss: 0.002248 \n",
      "Step:   69   |   Loss: 0.002678 \n",
      "Step:   71   |   Loss: 0.000082 \n",
      "Step:   72   |   Loss: 0.000570 \n",
      "Step:   73   |   Loss: 0.001025 \n",
      "Step:   74   |   Loss: 0.000357 \n",
      "Step:   75   |   Loss: 0.000156 \n",
      "Step:   76   |   Loss: 0.000669 \n",
      "Step:   77   |   Loss: 0.000118 \n",
      "Step:   78   |   Loss: 0.000644 \n",
      "Step:   79   |   Loss: 0.001599 \n",
      "Step:   81   |   Loss: 0.000546 \n",
      "Step:   82   |   Loss: 0.001871 \n",
      "Step:   83   |   Loss: 0.000555 \n",
      "Step:   84   |   Loss: 0.001042 \n",
      "Step:   85   |   Loss: 0.003329 \n",
      "Step:   86   |   Loss: 0.001288 \n",
      "Step:   87   |   Loss: 0.001016 \n",
      "Step:   88   |   Loss: 0.003789 \n",
      "Step:   89   |   Loss: 0.001655 \n",
      "Step:   91   |   Loss: 0.004252 \n",
      "Step:   92   |   Loss: 0.002498 \n",
      "Step:   93   |   Loss: 0.000376 \n",
      "Step:   94   |   Loss: 0.002753 \n",
      "Step:   95   |   Loss: 0.002029 \n",
      "Step:   96   |   Loss: 0.000147 \n",
      "Step:   97   |   Loss: 0.001866 \n",
      "Step:   98   |   Loss: 0.001915 \n",
      "Step:   99   |   Loss: 0.000133 \n",
      "Step:  101   |   Loss: 0.001681 \n",
      "Step:  102   |   Loss: 0.000182 \n",
      "Step:  103   |   Loss: 0.001090 \n",
      "Step:  104   |   Loss: 0.002204 \n",
      "Step:  105   |   Loss: 0.000406 \n",
      "Step:  106   |   Loss: 0.000631 \n",
      "Step:  107   |   Loss: 0.001510 \n",
      "Step:  108   |   Loss: 0.000257 \n",
      "Step:  109   |   Loss: 0.001653 \n",
      "Step:  111   |   Loss: 0.002717 \n",
      "Step:  112   |   Loss: 0.000254 \n",
      "Step:  113   |   Loss: 0.000692 \n",
      "Step:  114   |   Loss: 0.000863 \n",
      "Step:  115   |   Loss: 0.000378 \n",
      "Step:  116   |   Loss: 0.007099 \n",
      "Step:  117   |   Loss: 0.009987 \n",
      "Step:  118   |   Loss: 0.002177 \n",
      "Step:  119   |   Loss: 0.004794 \n",
      "Step:  121   |   Loss: 0.008111 \n",
      "Step:  122   |   Loss: 0.000150 \n",
      "Step:  123   |   Loss: 0.004513 \n",
      "Step:  124   |   Loss: 0.013758 \n",
      "Step:  125   |   Loss: 0.007697 \n",
      "Step:  126   |   Loss: 0.001560 \n",
      "Step:  127   |   Loss: 0.010749 \n",
      "Step:  128   |   Loss: 0.007452 \n",
      "Step:  129   |   Loss: 0.008016 \n",
      "Step:  131   |   Loss: 0.000966 \n",
      "Step:  132   |   Loss: 0.000993 \n",
      "Step:  133   |   Loss: 0.000544 \n",
      "Step:  134   |   Loss: 0.000719 \n",
      "Step:  135   |   Loss: 0.000334 \n",
      "Step:  136   |   Loss: 0.000203 \n",
      "Step:  137   |   Loss: 0.000337 \n",
      "Step:  138   |   Loss: 0.000387 \n",
      "Step:  139   |   Loss: 0.000488 \n",
      "Step:  141   |   Loss: 0.000106 \n",
      "Step:  142   |   Loss: 0.000180 \n",
      "Step:  143   |   Loss: 0.000132 \n",
      "Step:  144   |   Loss: 0.000217 \n",
      "Step:  145   |   Loss: 0.000035 \n",
      "Step:  146   |   Loss: 0.000035 \n",
      "Step:  147   |   Loss: 0.000002 \n",
      "Step:  148   |   Loss: 0.000008 \n",
      "Step:  149   |   Loss: 0.000007 \n",
      "Step:  151   |   Loss: 0.000005 \n",
      "Step:  152   |   Loss: 0.000007 \n",
      "Step:  153   |   Loss: 0.000005 \n",
      "Step:  154   |   Loss: 0.000009 \n",
      "Step:  155   |   Loss: 0.000008 \n",
      "Step:  156   |   Loss: 0.000043 \n",
      "Step:  157   |   Loss: 0.000005 \n",
      "Step:  158   |   Loss: 0.000005 \n",
      "Step:  159   |   Loss: 0.000004 \n",
      "Step:  161   |   Loss: 0.000127 \n",
      "Step:  162   |   Loss: 0.000081 \n",
      "Step:  163   |   Loss: 0.000016 \n",
      "Step:  164   |   Loss: 0.000015 \n",
      "Step:  165   |   Loss: 0.000079 \n",
      "Step:  166   |   Loss: 0.000934 \n",
      "Step:  167   |   Loss: 0.001009 \n",
      "Step:  168   |   Loss: 0.000078 \n",
      "Step:  169   |   Loss: 0.000179 \n",
      "Step:  171   |   Loss: 0.004375 \n",
      "Step:  172   |   Loss: 0.007433 \n",
      "Step:  173   |   Loss: 0.001456 \n",
      "Step:  174   |   Loss: 0.005578 \n",
      "Step:  175   |   Loss: 0.008137 \n",
      "Step:  176   |   Loss: 0.000949 \n",
      "Step:  177   |   Loss: 0.004253 \n",
      "Step:  178   |   Loss: 0.005866 \n",
      "Step:  179   |   Loss: 0.000872 \n",
      "Step:  181   |   Loss: 0.007661 \n",
      "Step:  182   |   Loss: 0.002207 \n",
      "Step:  183   |   Loss: 0.001308 \n",
      "Step:  184   |   Loss: 0.003499 \n",
      "Step:  185   |   Loss: 0.000380 \n",
      "Step:  186   |   Loss: 0.003987 \n",
      "Step:  187   |   Loss: 0.007717 \n",
      "Step:  188   |   Loss: 0.002742 \n",
      "Step:  189   |   Loss: 0.000571 \n",
      "Step:  191   |   Loss: 0.000094 \n",
      "Step:  192   |   Loss: 0.003495 \n",
      "Step:  193   |   Loss: 0.005249 \n",
      "Step:  194   |   Loss: 0.001281 \n",
      "Step:  195   |   Loss: 0.000833 \n",
      "Step:  196   |   Loss: 0.001676 \n",
      "Step:  197   |   Loss: 0.000139 \n",
      "Step:  198   |   Loss: 0.007831 \n",
      "Step:  199   |   Loss: 0.009006 \n",
      "Step:  201   |   Loss: 0.001995 \n",
      "Step:  202   |   Loss: 0.007969 \n",
      "Step:  203   |   Loss: 0.004465 \n",
      "Step:  204   |   Loss: 0.000130 \n",
      "Step:  205   |   Loss: 0.001627 \n",
      "Step:  206   |   Loss: 0.002300 \n",
      "Step:  207   |   Loss: 0.000172 \n",
      "Step:  208   |   Loss: 0.000234 \n",
      "Step:  209   |   Loss: 0.000286 \n",
      "Step:  211   |   Loss: 0.003880 \n",
      "Step:  212   |   Loss: 0.004167 \n",
      "Step:  213   |   Loss: 0.000461 \n",
      "Step:  214   |   Loss: 0.001600 \n",
      "Step:  215   |   Loss: 0.003323 \n",
      "Step:  216   |   Loss: 0.000525 \n",
      "Step:  217   |   Loss: 0.000497 \n",
      "Step:  218   |   Loss: 0.001350 \n",
      "Step:  219   |   Loss: 0.000031 \n",
      "Step:  221   |   Loss: 0.004159 \n",
      "Step:  222   |   Loss: 0.000751 \n",
      "Step:  223   |   Loss: 0.000975 \n",
      "Step:  224   |   Loss: 0.002778 \n",
      "Step:  225   |   Loss: 0.000365 \n",
      "Step:  226   |   Loss: 0.001062 \n",
      "Step:  227   |   Loss: 0.002900 \n",
      "Step:  228   |   Loss: 0.000402 \n",
      "Step:  229   |   Loss: 0.001139 \n",
      "Step:  231   |   Loss: 0.000602 \n",
      "Step:  232   |   Loss: 0.001125 \n",
      "Step:  233   |   Loss: 0.003661 \n",
      "Step:  234   |   Loss: 0.000883 \n",
      "Step:  235   |   Loss: 0.000873 \n",
      "Step:  236   |   Loss: 0.003643 \n",
      "Step:  237   |   Loss: 0.001201 \n",
      "Step:  238   |   Loss: 0.000661 \n",
      "Step:  239   |   Loss: 0.004085 \n",
      "Step:  241   |   Loss: 0.000252 \n",
      "Step:  242   |   Loss: 0.004029 \n",
      "Step:  243   |   Loss: 0.003634 \n",
      "Step:  244   |   Loss: 0.000136 \n",
      "Step:  245   |   Loss: 0.004889 \n",
      "Step:  246   |   Loss: 0.007524 \n",
      "Step:  247   |   Loss: 0.001055 \n",
      "Step:  248   |   Loss: 0.002659 \n",
      "Step:  249   |   Loss: 0.009155 \n",
      "Step:  251   |   Loss: 0.000124 \n",
      "Step:  252   |   Loss: 0.004076 \n",
      "Step:  253   |   Loss: 0.005994 \n",
      "Step:  254   |   Loss: 0.001750 \n",
      "Step:  255   |   Loss: 0.000200 \n",
      "Step:  256   |   Loss: 0.000115 \n",
      "Step:  257   |   Loss: 0.000238 \n",
      "Step:  258   |   Loss: 0.000119 \n",
      "Step:  259   |   Loss: 0.000038 \n",
      "Step:  261   |   Loss: 0.000023 \n",
      "Step:  262   |   Loss: 0.000183 \n",
      "Step:  263   |   Loss: 0.000950 \n",
      "Step:  264   |   Loss: 0.000376 \n",
      "Step:  265   |   Loss: 0.000019 \n",
      "Step:  266   |   Loss: 0.000507 \n",
      "Step:  267   |   Loss: 0.000014 \n",
      "Step:  268   |   Loss: 0.002091 \n",
      "Step:  269   |   Loss: 0.004890 \n",
      "Step:  271   |   Loss: 0.001799 \n",
      "Step:  272   |   Loss: 0.005307 \n",
      "Step:  273   |   Loss: 0.000158 \n",
      "Step:  274   |   Loss: 0.010284 \n",
      "Step:  275   |   Loss: 0.014988 \n",
      "Step:  276   |   Loss: 0.001267 \n",
      "Step:  277   |   Loss: 0.008220 \n",
      "Step:  278   |   Loss: 0.022050 \n",
      "Step:  279   |   Loss: 0.020478 \n",
      "Step:  281   |   Loss: 0.002696 \n",
      "Step:  282   |   Loss: 0.025692 \n",
      "Step:  283   |   Loss: 0.007931 \n",
      "Step:  284   |   Loss: 0.005394 \n",
      "Step:  285   |   Loss: 0.001510 \n",
      "Step:  286   |   Loss: 0.001774 \n",
      "Step:  287   |   Loss: 0.001624 \n",
      "Step:  288   |   Loss: 0.000323 \n",
      "Step:  289   |   Loss: 0.000220 \n",
      "Step:  291   |   Loss: 0.002121 \n",
      "Step:  292   |   Loss: 0.000325 \n",
      "Step:  293   |   Loss: 0.001165 \n",
      "Step:  294   |   Loss: 0.002087 \n",
      "Step:  295   |   Loss: 0.000192 \n",
      "Step:  296   |   Loss: 0.000284 \n",
      "Step:  297   |   Loss: 0.001389 \n",
      "Step:  298   |   Loss: 0.000719 \n",
      "Step:  299   |   Loss: 0.000158 \n",
      "Step:  301   |   Loss: 0.001421 \n",
      "Step:  302   |   Loss: 0.000915 \n",
      "Step:  303   |   Loss: 0.000066 \n",
      "Step:  304   |   Loss: 0.001203 \n",
      "Step:  305   |   Loss: 0.001842 \n",
      "Step:  306   |   Loss: 0.000130 \n",
      "Step:  307   |   Loss: 0.000577 \n",
      "Step:  308   |   Loss: 0.002229 \n",
      "Step:  309   |   Loss: 0.000808 \n",
      "Step:  311   |   Loss: 0.001783 \n",
      "Step:  312   |   Loss: 0.001640 \n",
      "Step:  313   |   Loss: 0.000226 \n",
      "Step:  314   |   Loss: 0.000887 \n",
      "Step:  315   |   Loss: 0.002107 \n",
      "Step:  316   |   Loss: 0.001005 \n",
      "Step:  317   |   Loss: 0.000180 \n",
      "Step:  318   |   Loss: 0.001957 \n",
      "Step:  319   |   Loss: 0.002052 \n",
      "Step:  321   |   Loss: 0.001400 \n",
      "Step:  322   |   Loss: 0.002930 \n",
      "Step:  323   |   Loss: 0.000435 \n",
      "Step:  324   |   Loss: 0.000722 \n",
      "Step:  325   |   Loss: 0.003245 \n",
      "Step:  326   |   Loss: 0.001242 \n",
      "Step:  327   |   Loss: 0.000188 \n",
      "Step:  328   |   Loss: 0.002806 \n",
      "Step:  329   |   Loss: 0.002227 \n",
      "Step:  331   |   Loss: 0.001805 \n",
      "Step:  332   |   Loss: 0.003070 \n",
      "Step:  333   |   Loss: 0.000781 \n",
      "Step:  334   |   Loss: 0.000729 \n",
      "Step:  335   |   Loss: 0.003471 \n",
      "Step:  336   |   Loss: 0.002198 \n",
      "Step:  337   |   Loss: 0.000073 \n",
      "Step:  338   |   Loss: 0.003369 \n",
      "Step:  339   |   Loss: 0.004276 \n",
      "Step:  341   |   Loss: 0.002685 \n",
      "Step:  342   |   Loss: 0.006620 \n",
      "Step:  343   |   Loss: 0.001442 \n",
      "Step:  344   |   Loss: 0.001156 \n",
      "Step:  345   |   Loss: 0.007830 \n",
      "Step:  346   |   Loss: 0.005445 \n",
      "Step:  347   |   Loss: 0.000364 \n",
      "Step:  348   |   Loss: 0.005500 \n",
      "Step:  349   |   Loss: 0.016848 \n",
      "Step:  351   |   Loss: 0.000333 \n",
      "Step:  352   |   Loss: 0.023101 \n",
      "Step:  353   |   Loss: 0.008925 \n",
      "Step:  354   |   Loss: 0.008209 \n",
      "Step:  355   |   Loss: 0.001145 \n",
      "Step:  356   |   Loss: 0.001833 \n",
      "Step:  357   |   Loss: 0.003647 \n",
      "Step:  358   |   Loss: 0.007550 \n",
      "Step:  359   |   Loss: 0.000719 \n",
      "Step:  361   |   Loss: 0.003677 \n",
      "Step:  362   |   Loss: 0.002664 \n",
      "Step:  363   |   Loss: 0.000277 \n",
      "Step:  364   |   Loss: 0.001607 \n",
      "Step:  365   |   Loss: 0.001260 \n",
      "Step:  366   |   Loss: 0.000250 \n",
      "Step:  367   |   Loss: 0.000265 \n",
      "Step:  368   |   Loss: 0.000994 \n",
      "Step:  369   |   Loss: 0.001325 \n",
      "Step:  371   |   Loss: 0.000659 \n",
      "Step:  372   |   Loss: 0.001524 \n",
      "Step:  373   |   Loss: 0.000280 \n",
      "Step:  374   |   Loss: 0.000121 \n",
      "Step:  375   |   Loss: 0.001180 \n",
      "Step:  376   |   Loss: 0.000726 \n",
      "Step:  377   |   Loss: 0.000107 \n",
      "Step:  378   |   Loss: 0.000483 \n",
      "Step:  379   |   Loss: 0.001162 \n",
      "Step:  381   |   Loss: 0.000048 \n",
      "Step:  382   |   Loss: 0.001263 \n",
      "Step:  383   |   Loss: 0.001824 \n",
      "Step:  384   |   Loss: 0.000075 \n",
      "Step:  385   |   Loss: 0.000784 \n",
      "Step:  386   |   Loss: 0.002271 \n",
      "Step:  387   |   Loss: 0.000611 \n",
      "Step:  388   |   Loss: 0.000136 \n",
      "Step:  389   |   Loss: 0.001835 \n",
      "Step:  391   |   Loss: 0.000383 \n",
      "Step:  392   |   Loss: 0.000893 \n",
      "Step:  393   |   Loss: 0.003135 \n",
      "Step:  394   |   Loss: 0.002213 \n",
      "Step:  395   |   Loss: 0.000150 \n",
      "Step:  396   |   Loss: 0.003809 \n",
      "Step:  397   |   Loss: 0.004619 \n",
      "Step:  398   |   Loss: 0.000089 \n",
      "Step:  399   |   Loss: 0.002916 \n",
      "Step:  401   |   Loss: 0.000983 \n",
      "Step:  402   |   Loss: 0.000891 \n",
      "Step:  403   |   Loss: 0.004552 \n",
      "Step:  404   |   Loss: 0.002777 \n",
      "Step:  405   |   Loss: 0.000249 \n",
      "Step:  406   |   Loss: 0.002134 \n",
      "Step:  407   |   Loss: 0.005753 \n",
      "Step:  408   |   Loss: 0.003479 \n",
      "Step:  409   |   Loss: 0.000383 \n",
      "Step:  411   |   Loss: 0.007292 \n",
      "Step:  412   |   Loss: 0.000392 \n",
      "Step:  413   |   Loss: 0.005531 \n",
      "Step:  414   |   Loss: 0.006164 \n",
      "Step:  415   |   Loss: 0.000975 \n",
      "Step:  416   |   Loss: 0.000794 \n",
      "Step:  417   |   Loss: 0.001541 \n",
      "Step:  418   |   Loss: 0.000413 \n",
      "Step:  419   |   Loss: 0.000165 \n",
      "Step:  421   |   Loss: 0.000198 \n",
      "Step:  422   |   Loss: 0.000063 \n",
      "Step:  423   |   Loss: 0.000083 \n",
      "Step:  424   |   Loss: 0.000053 \n",
      "Step:  425   |   Loss: 0.000058 \n",
      "Step:  426   |   Loss: 0.000014 \n",
      "Step:  427   |   Loss: 0.000042 \n",
      "Step:  428   |   Loss: 0.000088 \n",
      "Step:  429   |   Loss: 0.000006 \n",
      "Step:  431   |   Loss: 0.000099 \n",
      "Step:  432   |   Loss: 0.000008 \n",
      "Step:  433   |   Loss: 0.000017 \n",
      "Step:  434   |   Loss: 0.000085 \n",
      "Step:  435   |   Loss: 0.000014 \n",
      "Step:  436   |   Loss: 0.000014 \n",
      "Step:  437   |   Loss: 0.000057 \n",
      "Step:  438   |   Loss: 0.000026 \n",
      "Step:  439   |   Loss: 0.000022 \n",
      "Step:  441   |   Loss: 0.000038 \n",
      "Step:  442   |   Loss: 0.000049 \n",
      "Step:  443   |   Loss: 0.000017 \n",
      "Step:  444   |   Loss: 0.000043 \n",
      "Step:  445   |   Loss: 0.000084 \n",
      "Step:  446   |   Loss: 0.000007 \n",
      "Step:  447   |   Loss: 0.000037 \n",
      "Step:  448   |   Loss: 0.000116 \n",
      "Step:  449   |   Loss: 0.000005 \n",
      "Step:  451   |   Loss: 0.000126 \n",
      "Step:  452   |   Loss: 0.000007 \n",
      "Step:  453   |   Loss: 0.000022 \n",
      "Step:  454   |   Loss: 0.000112 \n",
      "Step:  455   |   Loss: 0.000006 \n",
      "Step:  456   |   Loss: 0.000021 \n",
      "Step:  457   |   Loss: 0.000083 \n",
      "Step:  458   |   Loss: 0.000008 \n",
      "Step:  459   |   Loss: 0.000032 \n",
      "Step:  461   |   Loss: 0.000037 \n",
      "Step:  462   |   Loss: 0.000043 \n",
      "Step:  463   |   Loss: 0.000005 \n",
      "Step:  464   |   Loss: 0.000112 \n",
      "Step:  465   |   Loss: 0.000023 \n",
      "Step:  466   |   Loss: 0.000058 \n",
      "Step:  467   |   Loss: 0.000168 \n",
      "Step:  468   |   Loss: 0.000043 \n",
      "Step:  469   |   Loss: 0.000305 \n",
      "Step:  471   |   Loss: 0.000486 \n",
      "Step:  472   |   Loss: 0.000251 \n",
      "Step:  473   |   Loss: 0.000406 \n",
      "Step:  474   |   Loss: 0.001042 \n",
      "Step:  475   |   Loss: 0.000108 \n",
      "Step:  476   |   Loss: 0.002065 \n",
      "Step:  477   |   Loss: 0.000194 \n",
      "Step:  478   |   Loss: 0.001591 \n",
      "Step:  479   |   Loss: 0.001427 \n",
      "Step:  481   |   Loss: 0.002445 \n",
      "Step:  482   |   Loss: 0.000027 \n",
      "Step:  483   |   Loss: 0.001699 \n",
      "Step:  484   |   Loss: 0.001343 \n",
      "Step:  485   |   Loss: 0.000207 \n",
      "Step:  486   |   Loss: 0.001516 \n",
      "Step:  487   |   Loss: 0.000476 \n",
      "Step:  488   |   Loss: 0.000291 \n",
      "Step:  489   |   Loss: 0.000918 \n",
      "Step:  491   |   Loss: 0.000270 \n",
      "Step:  492   |   Loss: 0.000580 \n",
      "Step:  493   |   Loss: 0.000048 \n",
      "Step:  494   |   Loss: 0.000332 \n",
      "Step:  495   |   Loss: 0.000262 \n",
      "Step:  496   |   Loss: 0.000027 \n",
      "Step:  497   |   Loss: 0.000403 \n",
      "Step:  498   |   Loss: 0.000010 \n",
      "Step:  499   |   Loss: 0.000330 \n"
     ]
    }
   ],
   "source": [
    "mv_net.train()\n",
    "#loss_list = []\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X),batch_size):\n",
    "        inpt = X[b:b+batch_size,:,:]\n",
    "        target = y[b:b+batch_size]\n",
    "        x_batch = torch.tensor(inpt,dtype=torch.float32)    \n",
    "        y_batch = torch.tensor(target,dtype=torch.float32)\n",
    "    \n",
    "        mv_net.init_hidden(x_batch.size(0))\n",
    "    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \n",
    "    #    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "        output = mv_net(x_batch) \n",
    "        loss = criterion(output, y_batch)  \n",
    "#         print('PREDICTED:\\n', output); print('REAL:\\n', y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        optimizer.zero_grad()\n",
    "        #loss_list.append(loss.item())\n",
    "    if t%10:\n",
    "        print(('Step: {:4}   |   Loss: {:.6f} ').format(t, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([0.7008])\n",
      "Real: [0.66802083]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('Predicted:', mv_net(torch.tensor(X[4:7,:,:],dtype=torch.float32))[0])\n",
    "    print('Real:', y[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the predicted time to send the message in a file, so that the Daemon can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected time to go to sleep:  2020-11-11 02:16:33\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "with torch.no_grad():\n",
    "    p = mv_net.forward(torch.tensor(X[-batch_size-1:-1:,:],dtype=torch.float32))[0].numpy()\n",
    "p_sec = int(p[0]*(max_hour+24-min_hour)*3600)\n",
    "prediction = now.replace(hour=min_hour, minute=0, second=0) + timedelta(seconds=p_sec)\n",
    "print('Expected time to go to sleep: ', prediction.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "\n",
    "'''Write the value on a text file to be read by the Daemon'''\n",
    "with open ('prediction.txt','w') as f:\n",
    "    f.writelines(prediction.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
